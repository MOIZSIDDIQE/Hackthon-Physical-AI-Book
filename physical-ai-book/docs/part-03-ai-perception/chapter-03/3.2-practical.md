---
title: 3.2 Audio Perception and Speech Processing Implementation
sidebar_position: 8
---

# 3.2 Audio Perception and Speech Processing Implementation

## Learning Objectives
- Implement audio perception systems for real humanoid robots
- Understand microphone array integration and calibration
- Apply speech recognition and sound processing algorithms
- Develop practical implementations using ROS and Python
- Integrate audio with navigation and social interaction

## Introduction

Implementing audio perception for humanoid robots requires careful consideration of hardware constraints, real-time processing requirements, and system integration. Unlike theoretical audio processing algorithms, practical implementation must handle microphone noise, acoustic reflections, computational limitations, and the need for robust operation in dynamic environments. This section provides practical guidance on implementing audio perception systems for humanoid robots, with specific focus on software frameworks, sensor integration, and real-world deployment considerations.

The implementation of audio perception for humanoid robots involves several key components: microphone drivers and array processing, real-time signal processing pipelines, speech recognition model deployment, sound source localization, and integration with higher-level interaction and navigation systems. Modern humanoid robots like Pepper, NAO, and HRP-4 demonstrate successful implementation of complex audio systems for tasks ranging from speech recognition to environmental sound classification.

## 1. Microphone Array Integration and Processing

### Microphone Driver Implementation

```python
import pyaudio
import numpy as np
import rospy
from std_msgs.msg import String
from audio_common_msgs.msg import AudioData
import threading

class MicrophoneArray:
    def __init__(self, num_channels=4, sample_rate=16000, chunk_size=1024):
        self.num_channels = num_channels
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.audio = pyaudio.PyAudio()

        # Initialize microphone array
        self.stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=num_channels,
            rate=sample_rate,
            input=True,
            frames_per_buffer=chunk_size,
            input_device_index=0  # Use default device
        )

        # Publishers for audio data
        self.audio_pub = rospy.Publisher('/audio/raw', AudioData, queue_size=10)
        self.speech_pub = rospy.Publisher('/audio/speech_detected', String, queue_size=10)

        # Audio processing parameters
        self.energy_threshold = 1000  # Threshold for speech detection
        self.silence_threshold = 500  # Threshold for silence
        self.silence_duration = 1.0   # Duration to consider speech ended

        # Threading for continuous processing
        self.is_running = True
        self.processing_thread = threading.Thread(target=self.process_audio)
        self.processing_thread.daemon = True

    def start(self):
        """
        Start audio capture and processing
        """
        self.processing_thread.start()

    def stop(self):
        """
        Stop audio capture and processing
        """
        self.is_running = False
        self.processing_thread.join()
        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()

    def process_audio(self):
        """
        Continuous audio processing loop
        """
        while self.is_running:
            # Read audio data
            data = self.stream.read(self.chunk_size, exception_on_overflow=False)
            audio_array = np.frombuffer(data, dtype=np.int16)

            # Separate channels
            if self.num_channels > 1:
                audio_channels = audio_array.reshape(-1, self.num_channels).T
            else:
                audio_channels = audio_array

            # Publish raw audio data
            audio_msg = AudioData()
            audio_msg.data = data
            audio_msg.header.stamp = rospy.Time.now()
            self.audio_pub.publish(audio_msg)

            # Detect speech activity
            speech_active = self.detect_speech(audio_channels)
            if speech_active:
                self.speech_pub.publish("SPEECH_DETECTED")

    def detect_speech(self, audio_data):
        """
        Simple voice activity detection based on energy
        """
        if len(audio_data.shape) > 1:
            # Use first channel for VAD
            audio_signal = audio_data[0]
        else:
            audio_signal = audio_data

        # Calculate energy in the frame
        energy = np.mean(audio_signal ** 2)

        # Simple threshold-based VAD
        return energy > self.energy_threshold
```

### Beamforming Implementation

```python
import numpy as np
from scipy import signal

class Beamformer:
    def __init__(self, num_mics=4, mic_distance=0.05, sample_rate=16000):
        self.num_mics = num_mics
        self.mic_distance = mic_distance
        self.sample_rate = sample_rate
        self.speed_of_sound = 343.0  # m/s

        # Pre-allocate arrays for efficiency
        self.delay_buffer = np.zeros((num_mics, 1024))  # Buffer for delays
        self.weights = np.ones(num_mics, dtype=complex)  # Beamforming weights

    def calculate_delays(self, angle_deg):
        """
        Calculate delays for beamforming in a specific direction
        """
        angle_rad = np.deg2rad(angle_deg)

        # For linear array, delays depend on angle
        delays = []
        for i in range(self.num_mics):
            distance_offset = i * self.mic_distance
            time_delay = (distance_offset * np.sin(angle_rad)) / self.speed_of_sound
            sample_delay = int(time_delay * self.sample_rate)
            delays.append(sample_delay)

        return np.array(delays)

    def delay_and_sum(self, audio_signals, delays):
        """
        Apply delay-and-sum beamforming
        """
        # Ensure signals are numpy arrays
        if not isinstance(audio_signals, np.ndarray):
            audio_signals = np.array(audio_signals)

        # Apply delays to each channel
        delayed_signals = []
        max_delay = max(delays) if len(delays) > 0 else 0

        for i, delay in enumerate(delays):
            if i < audio_signals.shape[0]:
                sig = audio_signals[i]
                if delay > 0:
                    # Pad with zeros for positive delay
                    delayed_sig = np.concatenate([np.zeros(delay), sig[:-delay]]) if len(sig) > delay else np.zeros_like(sig)
                else:
                    delayed_sig = sig
                delayed_signals.append(delayed_sig)

        if delayed_signals:
            # Sum all delayed signals
            beamformed = np.sum(delayed_signals, axis=0)
            return beamformed
        else:
            return np.zeros_like(audio_signals[0]) if audio_signals.size > 0 else np.array([])

    def mvdr_beamformer(self, audio_signals, steering_vector):
        """
        Minimum Variance Distortionless Response beamformer
        """
        # Calculate covariance matrix
        R_xx = np.cov(audio_signals)

        # Calculate beamforming weights
        # w = R_xx^(-1) * steering_vector / (steering_vector^H * R_xx^(-1) * steering_vector)
        R_inv = np.linalg.inv(R_xx + 1e-6 * np.eye(R_xx.shape[0]))  # Add regularization

        numerator = np.dot(R_inv, steering_vector)
        denominator = np.dot(np.conj(steering_vector).T, np.dot(R_inv, steering_vector))

        weights = numerator / denominator

        # Apply beamforming
        output = np.dot(np.conj(weights).T, audio_signals)

        return output, weights
```

## 2. Speech Recognition Implementation

### Real-time Speech Recognition Pipeline

```python
import speech_recognition as sr
import threading
import queue
import time

class RealTimeSpeechRecognizer:
    def __init__(self, language='en-US', model_type='google'):
        self.language = language
        self.model_type = model_type
        self.recognizer = sr.Recognizer()

        # Adjust for ambient noise
        self.recognizer.energy_threshold = 4000  # Minimum audio energy to consider for recording
        self.recognizer.dynamic_energy_threshold = True

        # Audio source (will be set externally)
        self.audio_source = None

        # Processing queues
        self.audio_queue = queue.Queue()
        self.result_queue = queue.Queue()

        # Threading
        self.is_listening = False
        self.listening_thread = None

        # Callback functions
        self.speech_callbacks = []

    def set_audio_source(self, audio_source):
        """
        Set the audio source for recognition
        """
        self.audio_source = audio_source

    def add_speech_callback(self, callback):
        """
        Add callback function for speech recognition results
        """
        self.speech_callbacks.append(callback)

    def start_listening(self):
        """
        Start continuous listening for speech
        """
        if self.audio_source is None:
            raise ValueError("Audio source not set")

        self.is_listening = True
        self.listening_thread = threading.Thread(target=self._continuous_listen)
        self.listening_thread.daemon = True
        self.listening_thread.start()

    def stop_listening(self):
        """
        Stop continuous listening
        """
        self.is_listening = False
        if self.listening_thread:
            self.listening_thread.join()

    def _continuous_listen(self):
        """
        Internal method for continuous speech recognition
        """
        with self.audio_source as source:
            while self.is_listening:
                try:
                    # Listen for audio with timeout
                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)

                    if self.is_listening:
                        # Process audio in background to avoid blocking
                        threading.Thread(
                            target=self._process_audio,
                            args=(audio,),
                            daemon=True
                        ).start()

                except sr.WaitTimeoutError:
                    # No speech detected, continue listening
                    continue
                except Exception as e:
                    rospy.logerr(f"Speech recognition error: {e}")
                    time.sleep(0.1)  # Brief pause before continuing

    def _process_audio(self, audio):
        """
        Process audio and perform recognition
        """
        try:
            if self.model_type == 'google':
                text = self.recognizer.recognize_google(audio, language=self.language)
            elif self.model_type == 'wit':
                text = self.recognizer.recognize_wit(audio, key="YOUR_WIT_AI_KEY")
            else:
                text = self.recognizer.recognize_sphinx(audio)  # Offline option

            # Call all registered callbacks
            for callback in self.speech_callbacks:
                callback(text)

        except sr.UnknownValueError:
            # Speech was detected but not understood
            rospy.logdebug("Speech not understood")
        except sr.RequestError as e:
            rospy.logerr(f"Speech recognition request error: {e}")

# Advanced speech recognition using deep learning models
class DeepSpeechRecognizer:
    def __init__(self, model_path=None):
        try:
            import deepspeech
            if model_path:
                self.model = deepspeech.Model(model_path)
            else:
                # Use default model if available
                self.model = None
        except ImportError:
            rospy.logwarn("Deepspeech not available, using basic recognizer")
            self.model = None

        self.sample_rate = 16000

    def recognize_audio(self, audio_data):
        """
        Recognize speech using deep learning model
        """
        if self.model is None:
            return "Deepspeech model not available"

        # Convert audio data to appropriate format
        # Deepspeech expects 16-bit mono audio at 16kHz
        if len(audio_data.shape) > 1:
            # Take first channel if multi-channel
            audio_mono = audio_data[0]
        else:
            audio_mono = audio_data

        # Ensure correct sample rate and format
        audio_int16 = (audio_mono * 32767).astype(np.int16)

        # Perform recognition
        text = self.model.stt(audio_int16)
        return text
```

## 3. Sound Source Localization Implementation

### GCC-PHAT Algorithm

```python
import numpy as np
from scipy import signal
from scipy.fftpack import fft, ifft

class SoundSourceLocalizer:
    def __init__(self, mic_positions, sample_rate=16000):
        """
        Initialize sound source localizer
        mic_positions: array of microphone positions [num_mics, 3]
        """
        self.mic_positions = np.array(mic_positions)
        self.sample_rate = sample_rate
        self.speed_of_sound = 343.0  # m/s

        # Pre-allocate for efficiency
        self.frame_size = 1024
        self.fft_size = 2048

    def gcc_phat(self, signal1, signal2):
        """
        Generalized Cross Correlation with Phase Transform
        """
        # Compute FFTs
        fft1 = fft(signal1, self.fft_size)
        fft2 = fft(signal2, self.fft_size)

        # Compute cross-spectrum
        cross_spectrum = fft1 * np.conj(fft2)

        # Phase transform
        phat = cross_spectrum / (np.abs(cross_spectrum) + 1e-12)  # Add small value to avoid division by zero

        # Compute GCC
        gcc_phat_result = np.real(ifft(phat))

        return gcc_phat_result

    def estimate_tdoa(self, signals):
        """
        Estimate Time Difference of Arrival between microphone pairs
        signals: array of signals from each microphone [num_mics, signal_length]
        """
        num_mics = signals.shape[0]
        tdof_matrix = np.zeros((num_mics, num_mics))

        for i in range(num_mics):
            for j in range(i+1, num_mics):
                gcc_result = self.gcc_phat(signals[i], signals[j])

                # Find peak in GCC (time delay)
                delay_index = np.argmax(np.abs(gcc_result))

                # Convert to time delay
                if delay_index > len(gcc_result) // 2:
                    delay_index -= len(gcc_result)

                tdoa = delay_index / self.sample_rate
                tdof_matrix[i, j] = tdoa
                tdof_matrix[j, i] = -tdoa  # Opposite delay

        return tdof_matrix

    def estimate_direction(self, signals):
        """
        Estimate direction of sound source using TDOA
        """
        # Get TDOA matrix
        tdoa_matrix = self.estimate_tdoa(signals)

        # For a linear array, we can estimate azimuth angle
        # This is a simplified approach for linear arrays
        if self.mic_positions.shape[0] >= 2:
            # Use first two microphones for simple angle estimation
            dx = self.mic_positions[1, 0] - self.mic_positions[0, 0]
            dt = tdoa_matrix[0, 1]

            # Calculate angle (simplified for linear array)
            if dt != 0:
                angle_rad = np.arcsin((dt * self.speed_of_sound) / dx)
                angle_deg = np.rad2deg(angle_rad)
                return angle_deg
            else:
                return 0.0  # Directly ahead

        return 0.0  # Default direction
```

## 4. ROS Implementation for Audio Perception

### Audio Processing Node

```python
#!/usr/bin/env python3

import rospy
import numpy as np
from audio_common_msgs.msg import AudioData
from std_msgs.msg import String, Float32
from geometry_msgs.msg import Point
from humanoid_audio.msg import AudioLocalization, SpeechRecognition

class AudioPerceptionNode:
    def __init__(self):
        rospy.init_node('audio_perception_node')

        # Initialize audio components
        self.microphone_array = MicrophoneArray(num_channels=4)
        self.beamformer = Beamformer()
        self.speech_recognizer = RealTimeSpeechRecognizer()
        self.sound_localizer = SoundSourceLocalizer(
            mic_positions=[[0, 0, 0], [0.1, 0, 0], [0.1, 0.1, 0], [0, 0.1, 0]]
        )

        # Publishers
        self.localization_pub = rospy.Publisher('/audio/localization', AudioLocalization, queue_size=10)
        self.recognition_pub = rospy.Publisher('/audio/recognition', SpeechRecognition, queue_size=10)
        self.direction_pub = rospy.Publisher('/audio/direction', Point, queue_size=10)

        # Subscribers
        self.audio_sub = rospy.Subscriber('/audio/raw', AudioData, self.audio_callback)

        # Processing parameters
        self.processing_rate = rospy.Rate(10)  # 10 Hz processing
        self.frame_size = 1024
        self.overlap = 512

        # Audio buffer for continuous processing
        self.audio_buffer = np.zeros(2048)
        self.buffer_index = 0

        # Start microphone array
        self.microphone_array.start()

        # Register speech recognition callback
        self.speech_recognizer.add_speech_callback(self.speech_recognized_callback)

    def audio_callback(self, audio_msg):
        """
        Receive and process audio data
        """
        try:
            # Convert audio data to numpy array
            audio_data = np.frombuffer(audio_msg.data, dtype=np.int16)

            # Process multi-channel audio
            if len(audio_data) % 4 == 0:  # Assuming 4 channels
                audio_channels = audio_data.reshape(-1, 4).T
            else:
                # Handle single channel or other configurations
                audio_channels = audio_data

            # Perform sound source localization
            if audio_channels.ndim > 1 and audio_channels.shape[0] >= 2:
                direction_estimate = self.sound_localizer.estimate_direction(audio_channels)

                # Create localization message
                loc_msg = AudioLocalization()
                loc_msg.header = audio_msg.header
                loc_msg.azimuth = direction_estimate
                loc_msg.confidence = 0.8  # Placeholder confidence

                self.localization_pub.publish(loc_msg)

                # Publish direction as Point (for visualization)
                dir_point = Point()
                dir_point.x = np.cos(np.deg2rad(direction_estimate))
                dir_point.y = np.sin(np.deg2rad(direction_estimate))
                dir_point.z = 0.0
                self.direction_pub.publish(dir_point)

            # Add to processing buffer
            self.add_to_buffer(audio_data)

        except Exception as e:
            rospy.logerr(f"Error processing audio: {e}")

    def add_to_buffer(self, audio_data):
        """
        Add audio data to processing buffer
        """
        data_len = len(audio_data)

        if self.buffer_index + data_len <= len(self.audio_buffer):
            self.audio_buffer[self.buffer_index:self.buffer_index + data_len] = audio_data
            self.buffer_index += data_len
        else:
            # Buffer overflow, reset
            remaining = len(self.audio_buffer) - self.buffer_index
            if remaining > 0:
                self.audio_buffer[self.buffer_index:] = audio_data[:remaining]
            self.buffer_index = 0
            # Add remaining data to beginning
            if remaining < data_len:
                self.audio_buffer[:data_len - remaining] = audio_data[remaining:]

    def speech_recognized_callback(self, text):
        """
        Callback for speech recognition results
        """
        rospy.loginfo(f"Recognized speech: {text}")

        # Create recognition message
        rec_msg = SpeechRecognition()
        rec_msg.header.stamp = rospy.Time.now()
        rec_msg.transcript = text
        rec_msg.confidence = 0.9  # Placeholder confidence

        self.recognition_pub.publish(rec_msg)

        # Trigger appropriate robot response based on recognized text
        self.process_speech_command(text)

    def process_speech_command(self, text):
        """
        Process recognized speech commands
        """
        text_lower = text.lower()

        # Simple command recognition
        if 'hello' in text_lower or 'hi' in text_lower:
            rospy.loginfo("Hello command detected")
            # Trigger robot greeting behavior
        elif 'stop' in text_lower:
            rospy.loginfo("Stop command detected")
            # Trigger robot stop behavior
        elif 'help' in text_lower:
            rospy.loginfo("Help command detected")
            # Trigger robot help behavior
        else:
            rospy.loginfo(f"Unknown command: {text}")

    def run(self):
        """
        Main processing loop
        """
        rospy.loginfo("Audio perception node started")

        try:
            while not rospy.is_shutdown():
                # Perform any continuous audio processing here
                # For example, continuous sound classification

                self.processing_rate.sleep()

        except rospy.ROSInterruptException:
            pass
        finally:
            # Cleanup
            self.microphone_array.stop()

if __name__ == '__main__':
    try:
        audio_node = AudioPerceptionNode()
        audio_node.run()
    except rospy.ROSInterruptException:
        pass
```

## 5. Environmental Sound Classification

### Sound Classification Implementation

```python
import librosa
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

class EnvironmentalSoundClassifier:
    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100)
        self.scaler = StandardScaler()
        self.is_trained = False

        # Sound classes
        self.classes = [
            'silence', 'speech', 'door_slam', 'phone_ring',
            'footsteps', 'applause', 'cough', 'sneeze'
        ]

    def extract_features(self, audio_signal, sr=16000):
        """
        Extract features for environmental sound classification
        """
        # MFCC features
        mfccs = librosa.feature.mfcc(y=audio_signal, sr=sr, n_mfcc=13)
        mfcc_mean = np.mean(mfccs, axis=1)
        mfcc_std = np.std(mfccs, axis=1)

        # Spectral features
        spectral_centroids = librosa.feature.spectral_centroid(y=audio_signal, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_signal, sr=sr)[0]
        zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_signal)[0]

        # Aggregate features
        features = np.concatenate([
            mfcc_mean,
            mfcc_std,
            [np.mean(spectral_centroids), np.std(spectral_centroids)],
            [np.mean(spectral_rolloff), np.std(spectral_rolloff)],
            [np.mean(zero_crossing_rate), np.std(zero_crossing_rate)]
        ])

        return features

    def train(self, audio_samples, labels):
        """
        Train the classifier with audio samples and labels
        """
        features_list = []

        for audio in audio_samples:
            features = self.extract_features(audio)
            features_list.append(features)

        X = np.array(features_list)
        y = np.array(labels)

        # Scale features
        X_scaled = self.scaler.fit_transform(X)

        # Train model
        self.model.fit(X_scaled, y)
        self.is_trained = True

    def classify_sound(self, audio_signal):
        """
        Classify environmental sound
        """
        if not self.is_trained:
            return 'unknown'

        features = self.extract_features(audio_signal)
        features_scaled = self.scaler.transform(features.reshape(1, -1))

        prediction = self.model.predict(features_scaled)[0]
        probability = np.max(self.model.predict_proba(features_scaled))

        return prediction, probability

# Integration with main audio system
class IntegratedAudioSystem:
    def __init__(self):
        self.microphone_array = MicrophoneArray()
        self.speech_recognizer = RealTimeSpeechRecognizer()
        self.sound_classifier = EnvironmentalSoundClassifier()
        self.sound_localizer = SoundSourceLocalizer(
            mic_positions=[[0, 0, 0], [0.1, 0, 0], [0.1, 0.1, 0], [0, 0.1, 0]]
        )

        # Initialize with some training data (in practice, load pre-trained model)
        # self.load_pretrained_models()

    def process_audio_frame(self, audio_data):
        """
        Process a frame of audio data for all tasks
        """
        results = {}

        # Speech detection and recognition
        if self.is_speech(audio_data):
            results['speech'] = self.speech_recognizer.recognize_audio(audio_data)
        else:
            # Environmental sound classification
            sound_class, confidence = self.sound_classifier.classify_sound(audio_data)
            results['environmental_sound'] = {
                'class': sound_class,
                'confidence': confidence
            }

        # Sound source localization
        direction = self.sound_localizer.estimate_direction(audio_data)
        results['direction'] = direction

        return results

    def is_speech(self, audio_data):
        """
        Simple speech detection based on energy and zero-crossing rate
        """
        energy = np.mean(audio_data ** 2)
        zcr = np.mean(np.abs(np.diff(np.sign(audio_data)))) / 2

        # Simple threshold-based detection
        return energy > 1000 and 50 < zcr < 100  # Adjust thresholds as needed
```

## 6. Performance Optimization

### Efficient Audio Processing Pipeline

```python
import threading
import queue
from collections import deque

class OptimizedAudioPipeline:
    def __init__(self, num_threads=2):
        self.input_queue = queue.Queue(maxsize=10)
        self.result_queue = queue.Queue(maxsize=10)

        # Processing components
        self.speech_detector = self.is_speech
        self.feature_extractor = self.extract_features
        self.classifier = EnvironmentalSoundClassifier()

        # Threading
        self.processing_threads = []
        self.is_running = True

        # Start processing threads
        for i in range(num_threads):
            thread = threading.Thread(target=self.process_loop, args=(i,))
            thread.daemon = True
            thread.start()
            self.processing_threads.append(thread)

    def process_loop(self, thread_id):
        """
        Processing loop for each thread
        """
        while self.is_running:
            try:
                audio_data, timestamp = self.input_queue.get(timeout=1.0)

                # Process audio
                features = self.feature_extractor(audio_data)

                # Classify based on features
                if self.speech_detector(audio_data):
                    result = {
                        'type': 'speech',
                        'features': features,
                        'timestamp': timestamp
                    }
                else:
                    pred, conf = self.classifier.classify_sound(audio_data)
                    result = {
                        'type': 'sound',
                        'class': pred,
                        'confidence': conf,
                        'timestamp': timestamp
                    }

                # Put result in output queue
                self.result_queue.put(result)

            except queue.Empty:
                continue
            except Exception as e:
                rospy.logerr(f"Processing error in thread {thread_id}: {e}")

    def add_audio(self, audio_data, timestamp):
        """
        Add audio data for processing
        """
        try:
            self.input_queue.put_nowait((audio_data, timestamp))
        except queue.Full:
            # Drop oldest if queue is full
            try:
                self.input_queue.get_nowait()
                self.input_queue.put_nowait((audio_data, timestamp))
            except queue.Empty:
                pass

    def get_results(self):
        """
        Get processed results
        """
        results = []
        try:
            while True:
                result = self.result_queue.get_nowait()
                results.append(result)
        except queue.Empty:
            pass

        return results

    def extract_features(self, audio_data):
        """
        Extract features efficiently
        """
        # Simplified feature extraction for real-time performance
        energy = np.mean(audio_data ** 2)
        zcr = np.mean(np.abs(np.diff(np.sign(audio_data)))) / 2
        rms = np.sqrt(np.mean(audio_data ** 2))

        return np.array([energy, zcr, rms])

    def is_speech(self, audio_data):
        """
        Fast speech detection
        """
        energy = np.mean(audio_data ** 2)
        zcr = np.mean(np.abs(np.diff(np.sign(audio_data)))) / 2

        return energy > 1000 and 50 < zcr < 100
```

## 7. Hardware Integration

### Audio Hardware Abstraction Layer

```python
import alsaaudio  # For Linux audio
import sounddevice as sd  # Cross-platform audio

class AudioHardwareInterface:
    def __init__(self, device_name=None):
        self.device_name = device_name
        self.sample_rate = 16000
        self.channels = 4
        self.chunk_size = 1024

        # Try to initialize audio device
        try:
            # Try ALSA first (Linux)
            self.audio_device = alsaaudio.PCM(alsaaudio.PCM_CAPTURE, alsaaudio.PCM_NORMAL)
            self.audio_device.setchannels(self.channels)
            self.audio_device.setrate(self.sample_rate)
            self.audio_device.setformat(alsaaudio.PCM_FORMAT_S16_LE)
            self.audio_device.setperiodsize(self.chunk_size)
            self.backend = 'alsa'
        except:
            # Fallback to sounddevice
            self.backend = 'sounddevice'
            self.stream = sd.InputStream(
                channels=self.channels,
                samplerate=self.sample_rate,
                blocksize=self.chunk_size,
                dtype='int16'
            )

    def read_audio(self):
        """
        Read audio data from hardware
        """
        if self.backend == 'alsa':
            length, data = self.audio_device.read()
            if length > 0:
                audio_array = np.frombuffer(data, dtype=np.int16)
                if self.channels > 1:
                    audio_array = audio_array.reshape(-1, self.channels).T
                return audio_array
        else:
            # For sounddevice, use callback-based approach
            audio_data, overflow = self.stream.read(self.chunk_size)
            return audio_data.T

    def get_device_info(self):
        """
        Get information about audio device
        """
        if self.backend == 'alsa':
            return {
                'backend': 'ALSA',
                'channels': self.channels,
                'sample_rate': self.sample_rate,
                'chunk_size': self.chunk_size
            }
        else:
            return {
                'backend': 'SoundDevice',
                'channels': self.channels,
                'sample_rate': self.sample_rate,
                'chunk_size': self.chunk_size
            }
```

## Conclusion

Implementing audio perception for humanoid robots requires careful integration of multiple technologies: microphone array processing, real-time signal processing, speech recognition, and system-level optimization. The examples provided demonstrate practical approaches to building robust audio systems that can handle the challenges of real-world deployment.

Success depends on balancing accuracy with real-time performance, managing computational resources efficiently, and ensuring robust operation in noisy environments. Modern tools like ROS, PyAudio, and speech recognition libraries provide the foundation for building sophisticated audio systems, while careful system design ensures they operate reliably on physical robots.

The field continues to evolve with advances in edge computing, specialized audio processing hardware, and more efficient algorithms, enabling increasingly capable audio perception systems for humanoid robots.