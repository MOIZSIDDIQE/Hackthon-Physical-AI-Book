---
title: 2.2 Computer Vision Implementation for Humanoid Robots
sidebar_position: 5
---

# 2.2 Computer Vision Implementation for Humanoid Robots

## Learning Objectives
- Implement computer vision systems for real humanoid robots
- Understand camera integration and calibration procedures
- Apply object detection and recognition algorithms
- Develop practical implementations using ROS and Python
- Integrate vision with navigation and manipulation

## Introduction

Implementing computer vision for humanoid robots requires careful consideration of hardware constraints, real-time processing requirements, and system integration. Unlike theoretical computer vision algorithms, practical implementation must handle sensor noise, calibration errors, computational limitations, and the need for robust operation in dynamic environments. This section provides practical guidance on implementing computer vision systems for humanoid robots, with specific focus on software frameworks, sensor integration, and real-world deployment considerations.

The implementation of computer vision for humanoid robots involves several key components: camera drivers and calibration, real-time processing pipelines, machine learning model deployment, and integration with higher-level planning and control systems. Modern humanoid robots like NAO, Pepper, and HRP-4 demonstrate successful implementation of complex vision systems for tasks ranging from object recognition to human interaction.

## 1. Camera Integration and Calibration

### Camera Driver Implementation

```python
import cv2
import numpy as np
import rospy
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge

class CameraDriver:
    def __init__(self, camera_id, camera_params):
        self.camera_id = camera_id
        self.bridge = CvBridge()

        # Camera parameters from calibration
        self.camera_matrix = np.array(camera_params['camera_matrix'])
        self.distortion_coeffs = np.array(camera_params['distortion_coeffs'])

        # Initialize camera
        self.cap = cv2.VideoCapture(camera_id)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_FPS, 30)

        # Publisher for camera images
        self.image_pub = rospy.Publisher(f'/camera/{camera_id}/image_raw', Image, queue_size=10)
        self.info_pub = rospy.Publisher(f'/camera/{camera_id}/camera_info', CameraInfo, queue_size=10)

        # Undistortion map (computed once for efficiency)
        self.mapx, self.mapy = cv2.initUndistortRectifyMap(
            self.camera_matrix, self.distortion_coeffs, None,
            self.camera_matrix, (640, 480), cv2.CV_32FC1
        )

    def capture_and_publish(self):
        """
        Capture frame and publish to ROS topic
        """
        ret, frame = self.cap.read()
        if ret:
            # Undistort the image
            undistorted = cv2.remap(frame, self.mapx, self.mapy, cv2.INTER_LINEAR)

            # Convert to ROS image message
            img_msg = self.bridge.cv2_to_imgmsg(undistorted, "bgr8")
            img_msg.header.stamp = rospy.Time.now()
            img_msg.header.frame_id = f"camera_{self.camera_id}_frame"

            # Publish image
            self.image_pub.publish(img_msg)

            # Publish camera info
            self.publish_camera_info(img_msg.header.stamp)

            return undistorted
        else:
            rospy.logwarn(f"Failed to capture frame from camera {self.camera_id}")
            return None

    def publish_camera_info(self, timestamp):
        """
        Publish camera calibration information
        """
        info_msg = CameraInfo()
        info_msg.header.stamp = timestamp
        info_msg.header.frame_id = f"camera_{self.camera_id}_frame"
        info_msg.height = 480
        info_msg.width = 640
        info_msg.K = self.camera_matrix.flatten().tolist()
        info_msg.D = self.distortion_coeffs.flatten().tolist()
        info_msg.R = np.eye(3).flatten().tolist()
        info_msg.P = np.zeros((3, 4)).flatten().tolist()
        info_msg.P[0] = self.camera_matrix[0, 0]  # fx
        info_msg.P[4] = self.camera_matrix[1, 1]  # fy
        info_msg.P[2] = self.camera_matrix[0, 2]  # cx
        info_msg.P[6] = self.camera_matrix[1, 2]  # cy
        info_msg.P[10] = 1.0  # P(2,2)

        self.info_pub.publish(info_msg)
```

### Multi-Camera Coordination

```python
import threading
from collections import deque

class MultiCameraManager:
    def __init__(self):
        self.cameras = {}
        self.frame_buffers = {}
        self.sync_lock = threading.Lock()

        # Initialize different cameras on the robot
        self.init_head_camera()
        self.init_hand_camera()
        self.init_body_camera()

        # Synchronization parameters
        self.max_sync_delay = 0.1  # 100ms max delay between cameras
        self.frame_buffer_size = 5

    def init_head_camera(self):
        """
        Initialize head-mounted camera for primary vision tasks
        """
        head_params = {
            'camera_matrix': [
                [525.0, 0.0, 319.5],
                [0.0, 525.0, 239.5],
                [0.0, 0.0, 1.0]
            ],
            'distortion_coeffs': [-0.3772, 0.1819, 0.0007, -0.0008, 0.0]
        }

        self.cameras['head'] = CameraDriver(0, head_params)
        self.frame_buffers['head'] = deque(maxlen=self.frame_buffer_size)

    def init_hand_camera(self):
        """
        Initialize hand-mounted camera for manipulation tasks
        """
        hand_params = {
            'camera_matrix': [
                [480.0, 0.0, 320.0],
                [0.0, 480.0, 240.0],
                [0.0, 0.0, 1.0]
            ],
            'distortion_coeffs': [-0.25, 0.1, 0.0, 0.0, 0.0]
        }

        self.cameras['hand'] = CameraDriver(1, hand_params)
        self.frame_buffers['hand'] = deque(maxlen=self.frame_buffer_size)

    def capture_all_cameras(self):
        """
        Synchronize capture from all cameras
        """
        with self.sync_lock:
            frames = {}

            # Capture from all cameras
            for name, camera in self.cameras.items():
                frame = camera.capture_and_publish()
                if frame is not None:
                    timestamp = rospy.Time.now()
                    frames[name] = {
                        'image': frame,
                        'timestamp': timestamp
                    }
                    # Add to buffer
                    self.frame_buffers[name].append({
                        'image': frame,
                        'timestamp': timestamp
                    })

            return frames

    def get_synchronized_frames(self, reference_camera='head', max_delay=0.1):
        """
        Get frames from all cameras synchronized to reference camera
        """
        if not self.frame_buffers[reference_camera]:
            return None

        ref_frame = self.frame_buffers[reference_camera][-1]
        ref_time = ref_frame['timestamp']

        synchronized_frames = {reference_camera: ref_frame}

        for cam_name in self.frame_buffers:
            if cam_name == reference_camera:
                continue

            # Find closest frame in time
            best_frame = None
            best_time_diff = float('inf')

            for frame in self.frame_buffers[cam_name]:
                time_diff = abs((frame['timestamp'] - ref_time).to_sec())
                if time_diff < best_time_diff and time_diff < max_delay:
                    best_time_diff = time_diff
                    best_frame = frame

            if best_frame:
                synchronized_frames[cam_name] = best_frame

        return synchronized_frames if len(synchronized_frames) == len(self.cameras) else None
```

## 2. Object Detection Implementation

### Real-time Object Detection Pipeline

```python
import torch
import torchvision.transforms as T
from torchvision.models.detection import fasterrcnn_resnet50_fpn
import time

class RealTimeObjectDetector:
    def __init__(self, model_path=None, confidence_threshold=0.5):
        self.confidence_threshold = confidence_threshold

        # Load pre-trained model
        if model_path:
            self.model = torch.load(model_path)
        else:
            self.model = fasterrcnn_resnet50_fpn(pretrained=True)

        self.model.eval()

        # Move to GPU if available
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            self.device = torch.device('cuda')
        else:
            self.device = torch.device('cpu')

        # COCO dataset class names
        self.coco_names = [
            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',
            'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',
            'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',
            'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',
            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',
            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',
            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',
            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',
            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',
            'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ]

        # Preprocessing transforms
        self.transform = T.Compose([
            T.ToTensor(),
        ])

        # Performance tracking
        self.processing_times = deque(maxlen=100)

    def preprocess_image(self, image):
        """
        Preprocess image for object detection
        """
        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Apply transforms
        image_tensor = self.transform(image_rgb)

        # Add batch dimension
        image_tensor = image_tensor.unsqueeze(0)

        # Move to device
        image_tensor = image_tensor.to(self.device)

        return image_tensor

    def detect_objects(self, image):
        """
        Detect objects in image with timing information
        """
        start_time = time.time()

        # Preprocess image
        image_tensor = self.preprocess_image(image)

        # Perform inference
        with torch.no_grad():
            predictions = self.model(image_tensor)

        # Move predictions to CPU for processing
        boxes = predictions[0]['boxes'].cpu().numpy()
        labels = predictions[0]['labels'].cpu().numpy()
        scores = predictions[0]['scores'].cpu().numpy()

        # Filter by confidence threshold
        valid_indices = scores > self.confidence_threshold
        boxes = boxes[valid_indices]
        labels = labels[valid_indices]
        scores = scores[valid_indices]

        # Package results
        detections = []
        for i in range(len(boxes)):
            detections.append({
                'bbox': boxes[i],  # [x1, y1, x2, y2]
                'label': self.coco_names[labels[i]],
                'confidence': scores[i],
                'class_id': labels[i]
            })

        # Track processing time
        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)

        return detections, processing_time

    def visualize_detections(self, image, detections):
        """
        Draw bounding boxes and labels on image
        """
        output_image = image.copy()

        for detection in detections:
            bbox = detection['bbox']
            label = detection['label']
            confidence = detection['confidence']

            # Draw bounding box
            cv2.rectangle(
                output_image,
                (int(bbox[0]), int(bbox[1])),
                (int(bbox[2]), int(bbox[3])),
                (0, 255, 0),
                2
            )

            # Draw label and confidence
            text = f"{label}: {confidence:.2f}"
            cv2.putText(
                output_image, text,
                (int(bbox[0]), int(bbox[1])-10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 255, 0),
                1
            )

        return output_image

    def get_performance_stats(self):
        """
        Get performance statistics
        """
        if not self.processing_times:
            return None

        avg_time = sum(self.processing_times) / len(self.processing_times)
        fps = 1.0 / avg_time if avg_time > 0 else 0

        return {
            'avg_processing_time': avg_time,
            'fps': fps,
            'min_time': min(self.processing_times),
            'max_time': max(self.processing_times)
        }
```

### Human Detection and Tracking

```python
import mediapipe as mp
import numpy as np

class HumanDetectorTracker:
    def __init__(self, max_num_humans=5):
        # Initialize MediaPipe solutions
        self.mp_pose = mp.solutions.pose
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_hands = mp.solutions.hands

        self.pose_detector = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            max_num_poses=max_num_humans
        )

        self.face_detector = self.mp_face_detection.FaceDetection(
            model_selection=0,
            min_detection_confidence=0.5
        )

        self.hand_detector = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.5
        )

        # Tracking state
        self.tracked_humans = {}
        self.next_id = 0

    def detect_humans(self, image):
        """
        Detect humans in image using multiple MediaPipe models
        """
        results = {}

        # Convert BGR to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Detect poses
        pose_results = self.pose_detector.process(image_rgb)
        results['poses'] = self.extract_pose_data(pose_results)

        # Detect faces
        face_results = self.face_detector.process(image_rgb)
        results['faces'] = self.extract_face_data(face_results)

        # Detect hands
        hand_results = self.hand_detector.process(image_rgb)
        results['hands'] = self.extract_hand_data(hand_results)

        return results

    def extract_pose_data(self, pose_results):
        """
        Extract pose data from MediaPipe results
        """
        if not pose_results.pose_landmarks:
            return []

        poses = []
        for pose_landmarks in pose_results.pose_landmarks:
            landmarks = []
            for landmark in pose_landmarks.landmark:
                landmarks.append({
                    'x': landmark.x,
                    'y': landmark.y,
                    'z': landmark.z,
                    'visibility': landmark.visibility
                })

            # Calculate bounding box from landmarks
            xs = [lm['x'] for lm in landmarks]
            ys = [lm['y'] for lm in landmarks]

            bbox = {
                'x_min': min(xs),
                'y_min': min(ys),
                'x_max': max(xs),
                'y_max': max(ys)
            }

            poses.append({
                'landmarks': landmarks,
                'bbox': bbox,
                'center': ((bbox['x_min'] + bbox['x_max']) / 2,
                          (bbox['y_min'] + bbox['y_max']) / 2)
            })

        return poses

    def track_humans(self, current_detections, max_distance=0.1):
        """
        Track humans across frames using centroid tracking
        """
        current_poses = current_detections['poses']

        # If no previous tracks, assign new IDs
        if not self.tracked_humans:
            for i, pose in enumerate(current_poses):
                self.tracked_humans[self.next_id] = {
                    'pose': pose,
                    'last_seen': rospy.Time.now(),
                    'velocity': np.array([0, 0])
                }
                self.next_id += 1
            return self.tracked_humans

        # Associate current detections with existing tracks
        unassigned_poses = list(range(len(current_poses)))
        updated_tracks = {}

        for track_id, track_data in self.tracked_humans.items():
            # Predict position based on velocity
            predicted_center = np.array(track_data['pose']['center']) + track_data['velocity']

            # Find closest unassigned detection
            best_match = None
            best_distance = float('inf')

            for i in unassigned_poses:
                current_center = np.array(current_poses[i]['center'])
                distance = np.linalg.norm(predicted_center - current_center)

                if distance < best_distance and distance < max_distance:
                    best_distance = distance
                    best_match = i

            if best_match is not None:
                # Update track
                velocity = current_center - np.array(track_data['pose']['center'])
                updated_tracks[track_id] = {
                    'pose': current_poses[best_match],
                    'last_seen': rospy.Time.now(),
                    'velocity': velocity * 0.3 + track_data['velocity'] * 0.7  # Smooth velocity
                }
                unassigned_poses.remove(best_match)
            else:
                # Check if track is still alive
                time_since_seen = (rospy.Time.now() - track_data['last_seen']).to_sec()
                if time_since_seen < 2.0:  # Keep track for up to 2 seconds
                    updated_tracks[track_id] = track_data

        # Add new tracks for unassigned detections
        for i in unassigned_poses:
            updated_tracks[self.next_id] = {
                'pose': current_poses[i],
                'last_seen': rospy.Time.now(),
                'velocity': np.array([0, 0])
            }
            self.next_id += 1

        self.tracked_humans = updated_tracks
        return self.tracked_humans
```

## 3. ROS Implementation for Vision

### Vision Processing Node

```python
#!/usr/bin/env python3

import rospy
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from std_msgs.msg import String
from geometry_msgs.msg import Point
from humanoid_vision.msg import DetectionArray, Detection
import threading

class VisionProcessingNode:
    def __init__(self):
        rospy.init_node('vision_processing_node')

        # Initialize components
        self.bridge = CvBridge()
        self.object_detector = RealTimeObjectDetector()
        self.human_detector = HumanDetectorTracker()
        self.multi_camera = MultiCameraManager()

        # Publishers
        self.detection_pub = rospy.Publisher('/vision/detections', DetectionArray, queue_size=10)
        self.visualization_pub = rospy.Publisher('/vision/visualization', Image, queue_size=10)
        self.performance_pub = rospy.Publisher('/vision/performance', String, queue_size=10)

        # Subscribers
        self.image_sub = rospy.Subscriber('/camera/head/image_raw', Image, self.image_callback)

        # Processing parameters
        self.processing_rate = rospy.Rate(10)  # 10 Hz
        self.frame_skip = 0  # Process every frame
        self.frame_count = 0

        # Threading for parallel processing
        self.processing_lock = threading.Lock()
        self.latest_image = None
        self.latest_detections = None

    def image_callback(self, img_msg):
        """
        Receive and store image for processing
        """
        try:
            with self.processing_lock:
                self.latest_image = self.bridge.imgmsg_to_cv2(img_msg, "bgr8")
                self.latest_header = img_msg.header
        except Exception as e:
            rospy.logerr(f"Error converting image: {e}")

    def process_latest_image(self):
        """
        Process the latest received image
        """
        with self.processing_lock:
            if self.latest_image is None:
                return None, None

            image = self.latest_image.copy()
            header = self.latest_header

        # Perform object detection
        detections, processing_time = self.object_detector.detect_objects(image)

        # Perform human detection
        human_results = self.human_detector.detect_humans(image)

        # Update human tracking
        tracked_humans = self.human_detector.track_humans(human_results)

        # Combine all detections
        all_detections = self.combine_detections(detections, tracked_humans)

        return all_detections, header, processing_time

    def combine_detections(self, object_detections, tracked_humans):
        """
        Combine object and human detections into unified format
        """
        combined = []

        # Add object detections
        for obj_det in object_detections:
            detection = Detection()
            detection.label = obj_det['label']
            detection.confidence = obj_det['confidence']
            detection.bbox.xmin = int(obj_det['bbox'][0])
            detection.bbox.ymin = int(obj_det['bbox'][1])
            detection.bbox.xmax = int(obj_det['bbox'][2])
            detection.bbox.ymax = int(obj_det['bbox'][3])
            detection.type = "object"
            combined.append(detection)

        # Add tracked humans
        for human_id, human_data in tracked_humans.items():
            detection = Detection()
            detection.label = f"person_{human_id}"
            detection.confidence = 0.9  # High confidence for tracked humans
            bbox = human_data['pose']['bbox']
            detection.bbox.xmin = int(bbox['x_min'] * 640)  # Scale to image coordinates
            detection.bbox.ymin = int(bbox['y_min'] * 480)
            detection.bbox.xmax = int(bbox['x_max'] * 640)
            detection.bbox.ymax = int(bbox['y_max'] * 480)
            detection.type = "person"
            detection.tracking_id = human_id
            combined.append(detection)

        return combined

    def publish_detections(self, detections, header):
        """
        Publish detection array to ROS topic
        """
        detection_array = DetectionArray()
        detection_array.header = header
        detection_array.detections = detections
        detection_array.frame_id = header.frame_id

        self.detection_pub.publish(detection_array)

    def visualize_and_publish(self, image, detections, header):
        """
        Visualize detections and publish result
        """
        # Visualize object detections
        vis_image = self.object_detector.visualize_detections(image, detections)

        # Convert back to ROS image
        vis_msg = self.bridge.cv2_to_imgmsg(vis_image, "bgr8")
        vis_msg.header = header

        self.visualization_pub.publish(vis_msg)

    def run(self):
        """
        Main processing loop
        """
        rospy.loginfo("Vision processing node started")

        while not rospy.is_shutdown():
            # Process latest image
            detections, header, processing_time = self.process_latest_image()

            if detections is not None:
                # Publish detections
                self.publish_detections(detections, header)

                # Visualize and publish
                image = self.latest_image
                self.visualize_and_publish(image, detections, header)

                # Publish performance stats
                perf_stats = self.object_detector.get_performance_stats()
                if perf_stats:
                    perf_str = f"FPS: {perf_stats['fps']:.1f}, Time: {perf_stats['avg_processing_time']*1000:.1f}ms"
                    perf_msg = String()
                    perf_msg.data = perf_str
                    self.performance_pub.publish(perf_msg)

            self.processing_rate.sleep()

if __name__ == '__main__':
    try:
        vision_node = VisionProcessingNode()
        vision_node.run()
    except rospy.ROSInterruptException:
        pass
```

## 4. Performance Optimization

### Efficient Processing Pipeline

```python
import queue
import threading
from concurrent.futures import ThreadPoolExecutor

class OptimizedVisionPipeline:
    def __init__(self, num_threads=4):
        self.executor = ThreadPoolExecutor(max_workers=num_threads)
        self.input_queue = queue.Queue(maxsize=10)
        self.result_queue = queue.Queue(maxsize=10)

        # Initialize processing components
        self.object_detector = RealTimeObjectDetector()
        self.human_detector = HumanDetectorTracker()

        # Processing thread
        self.processing_thread = threading.Thread(target=self.process_loop)
        self.processing_thread.daemon = True
        self.processing_thread.start()

        # Frame rate control
        self.target_fps = 15
        self.frame_time = 1.0 / self.target_fps

    def add_frame(self, image, timestamp):
        """
        Add frame to processing queue with frame dropping
        """
        try:
            self.input_queue.put_nowait((image, timestamp))
        except queue.Full:
            # Drop oldest frame if queue is full
            try:
                self.input_queue.get_nowait()
                self.input_queue.put_nowait((image, timestamp))
            except queue.Empty:
                pass

    def process_loop(self):
        """
        Continuous processing loop
        """
        while True:
            try:
                image, timestamp = self.input_queue.get(timeout=1.0)

                # Submit processing tasks
                obj_future = self.executor.submit(
                    self.object_detector.detect_objects, image
                )
                human_future = self.executor.submit(
                    self.human_detector.detect_humans, image
                )

                # Get results
                obj_detections, obj_time = obj_future.result()
                human_results = human_future.result()

                # Package results
                result = {
                    'object_detections': obj_detections,
                    'human_detections': human_results,
                    'timestamp': timestamp,
                    'processing_times': {
                        'objects': obj_time,
                        'humans': time.time() - timestamp.to_sec()  # Approximate
                    }
                }

                # Add to result queue
                try:
                    self.result_queue.put_nowait(result)
                except queue.Full:
                    # Drop oldest result if queue is full
                    try:
                        self.result_queue.get_nowait()
                        self.result_queue.put_nowait(result)
                    except queue.Empty:
                        pass

            except queue.Empty:
                continue
            except Exception as e:
                rospy.logerr(f"Processing error: {e}")

    def get_latest_results(self):
        """
        Get latest results from queue
        """
        results = []
        try:
            while True:
                result = self.result_queue.get_nowait()
                results.append(result)
        except queue.Empty:
            pass

        return results[-1] if results else None

    def process_frame_optimized(self, image):
        """
        Optimized single-frame processing
        """
        # Resize image if too large
        h, w = image.shape[:2]
        if w > 640 or h > 480:
            scale = min(640/w, 480/h)
            new_w, new_h = int(w*scale), int(h*scale)
            image = cv2.resize(image, (new_w, new_h))

        # Perform detection
        detections, proc_time = self.object_detector.detect_objects(image)

        return detections, proc_time
```

## 5. Hardware Integration

### GPU Acceleration

```python
import torch
import tensorrt as trt

class AcceleratedVision:
    def __init__(self, use_tensorrt=False):
        self.use_tensorrt = use_tensorrt
        self.use_gpu = torch.cuda.is_available()

        if self.use_gpu:
            self.device = torch.device('cuda')
            rospy.loginfo("Using GPU acceleration")
        else:
            self.device = torch.device('cpu')
            rospy.loginfo("Using CPU processing")

        # Load optimized model
        self.model = self.load_optimized_model()

    def load_optimized_model(self):
        """
        Load model with GPU optimization
        """
        if self.use_tensorrt:
            # Load TensorRT optimized model
            return self.load_tensorrt_model()
        else:
            # Load standard PyTorch model
            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(
                pretrained=True
            )
            model.eval()

            if self.use_gpu:
                model = model.cuda()

            return model

    def preprocess_for_gpu(self, image):
        """
        Preprocess image for GPU processing
        """
        # Convert to tensor and move to GPU
        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float().unsqueeze(0) / 255.0

        if self.use_gpu:
            image_tensor = image_tensor.cuda()

        return image_tensor

    def detect_gpu(self, image):
        """
        Perform detection using GPU acceleration
        """
        image_tensor = self.preprocess_for_gpu(image)

        start_time = time.time()

        with torch.no_grad():
            if self.use_gpu:
                torch.cuda.synchronize()  # Synchronize for accurate timing

            results = self.model(image_tensor)

            if self.use_gpu:
                torch.cuda.synchronize()  # Synchronize before returning results

        processing_time = time.time() - start_time

        return results, processing_time
```

## Conclusion

Implementing computer vision for humanoid robots requires careful integration of multiple technologies: camera calibration, real-time processing, machine learning model deployment, and system-level optimization. The examples provided demonstrate practical approaches to building robust vision systems that can handle the challenges of real-world deployment.

Success depends on balancing accuracy with real-time performance, managing computational resources efficiently, and ensuring robust operation in dynamic environments. Modern tools like ROS, PyTorch, and OpenCV provide the foundation for building sophisticated vision systems, while careful system design ensures they operate reliably on physical robots.

The field continues to evolve with advances in edge computing, specialized hardware, and more efficient algorithms, enabling increasingly capable vision systems for humanoid robots.