---
title: 6.8 Sensor Simulation Implementation
sidebar_position: 26
---

# 6.8 Sensor Simulation Implementation

## Learning Objectives
- Implement realistic sensor models for humanoid robots
- Design noise models and uncertainty quantification systems
- Integrate sensor simulation with physics and control systems
- Validate sensor simulation accuracy and performance
- Apply sensor simulation implementation to real-world scenarios

## Introduction

Sensor simulation implementation transforms theoretical concepts into executable code that can generate realistic sensor data for humanoid robots. This process involves not only implementing the core sensor models but also addressing practical challenges such as computational efficiency, noise modeling, and integration with existing robotic software stacks. The implementation must balance simulation accuracy with computational performance while ensuring that generated sensor data remains representative of real-world sensor behavior.

Effective sensor simulation implementation requires careful consideration of software architecture, numerical methods, and system integration. The sensor simulation system must interface with real-time operating systems, handle sensor noise and delays, and maintain consistency with physics simulation. Implementation choices significantly impact the performance, reliability, and maintainability of the resulting sensor simulation system.

This section provides practical examples of sensor simulation implementation using common frameworks and tools in robotics, with a focus on humanoid-specific challenges such as multi-modal sensing, sensor fusion, and real-time performance requirements.

## 2. Joint Encoder Simulation Implementation

### Encoder Model Implementation

A realistic encoder simulation model includes quantization, noise, and systematic errors:

```python
import numpy as np
import time
from scipy.spatial.transform import Rotation as R

class JointEncoder:
    def __init__(self, resolution=2**16, noise_std=1e-4, bias_drift_rate=1e-6,
                 scale_error=0.001, quantization_enabled=True):
        self.resolution = resolution  # Counts per revolution
        self.noise_std = noise_std    # Noise standard deviation in radians
        self.bias_drift_rate = bias_drift_rate  # Bias drift per second
        self.scale_error = scale_error  # Scale factor error
        self.quantization_enabled = quantization_enabled

        # State variables
        self.position = 0.0
        self.velocity = 0.0
        self.bias = 0.0
        self.last_update_time = time.time()

        # Bias drift simulation
        self.bias_drift = 0.0
        self.mounting_offset = np.random.normal(0, 1e-5)  # Small mounting error

    def update(self, true_position, true_velocity, current_time):
        """
        Update encoder simulation with true state
        """
        dt = current_time - self.last_update_time
        self.last_update_time = current_time

        # Apply systematic errors
        measured_position = (1 + self.scale_error) * (true_position + self.mounting_offset)

        # Add bias and bias drift
        self._update_bias_drift(dt)
        measured_position += self.bias + self.bias_drift

        # Add white noise
        noise = np.random.normal(0, self.noise_std)
        measured_position += noise

        # Apply quantization if enabled
        if self.quantization_enabled:
            measured_position = self._quantize(measured_position)

        # For velocity, apply similar error model
        measured_velocity = (1 + self.scale_error) * true_velocity
        measured_velocity += np.random.normal(0, self.noise_std * 10)  # Higher noise for velocity

        # Store results
        self.position = measured_position
        self.velocity = measured_velocity

        return {
            'position': measured_position,
            'velocity': measured_velocity,
            'timestamp': current_time,
            'bias': self.bias + self.bias_drift
        }

    def _quantize(self, position):
        """
        Simulate encoder quantization
        """
        # Convert to encoder counts
        counts = np.round(position * self.resolution / (2 * np.pi))
        # Convert back to radians
        quantized_position = counts * (2 * np.pi) / self.resolution
        return quantized_position

    def _update_bias_drift(self, dt):
        """
        Update bias drift using random walk model
        """
        drift_increment = np.random.normal(0, self.bias_drift_rate * np.sqrt(dt))
        self.bias_drift += drift_increment

    def reset_bias_drift(self):
        """
        Reset bias drift (simulates recalibration)
        """
        self.bias_drift = 0.0

class MultiJointEncoderSystem:
    def __init__(self, n_joints, resolutions=None, noise_stds=None):
        self.n_joints = n_joints

        if resolutions is None:
            resolutions = [2**16] * n_joints
        if noise_stds is None:
            noise_stds = [1e-4] * n_joints

        # Create encoder for each joint
        self.encoders = []
        for i in range(n_joints):
            encoder = JointEncoder(
                resolution=resolutions[i],
                noise_std=noise_stds[i]
            )
            self.encoders.append(encoder)

    def update_all(self, true_positions, true_velocities, current_time):
        """
        Update all encoders simultaneously
        """
        measurements = {
            'positions': np.zeros(self.n_joints),
            'velocities': np.zeros(self.n_joints),
            'biases': np.zeros(self.n_joints)
        }

        for i, encoder in enumerate(self.encoders):
            result = encoder.update(true_positions[i], true_velocities[i], current_time)
            measurements['positions'][i] = result['position']
            measurements['velocities'][i] = result['velocity']
            measurements['biases'][i] = result['bias']

        return measurements

    def get_measurement_covariance(self):
        """
        Get measurement covariance matrix for filtering applications
        """
        cov = np.zeros((2 * self.n_joints, 2 * self.n_joints))

        for i, encoder in enumerate(self.encoders):
            # Position variance
            pos_var = encoder.noise_std**2
            # Velocity variance (typically higher)
            vel_var = (encoder.noise_std * 10)**2

            # Position-position covariance
            cov[i, i] = pos_var
            # Velocity-velocity covariance
            cov[self.n_joints + i, self.n_joints + i] = vel_var

        return cov
```

## 3. IMU Sensor Simulation Implementation

### IMU Physics and Error Modeling

```python
class IMUSensor:
    def __init__(self, position=np.zeros(3), orientation=np.array([0, 0, 0, 1]),
                 noise_params=None, bias_params=None, calibration_params=None):
        # Physical mounting parameters
        self.position = position  # Position relative to body frame
        self.orientation = orientation  # Orientation relative to body frame (quaternion [w, x, y, z])

        # Noise parameters
        if noise_params is None:
            noise_params = {
                'gyro_noise_density': 1.0e-4,      # rad/s/sqrt(Hz)
                'gyro_random_walk': 1.0e-5,        # rad/s^2/sqrt(Hz)
                'accel_noise_density': 1.0e-3,     # m/s^2/sqrt(Hz)
                'accel_random_walk': 1.0e-4,       # m/s^3/sqrt(Hz)
                'mag_noise_density': 1.0e-6,       # Tesla/sqrt(Hz) (if magnetometer included)
            }
        self.noise_params = noise_params

        # Bias parameters
        if bias_params is None:
            bias_params = {
                'gyro_bias_init': np.random.normal(0, 1e-3, 3),  # Initial bias
                'gyro_bias_walk': 1.0e-6,           # Bias random walk
                'accel_bias_init': np.random.normal(0, 1e-2, 3), # Initial bias
                'accel_bias_walk': 1.0e-5,          # Bias random walk
            }
        self.bias_params = bias_params

        # Calibration parameters
        if calibration_params is None:
            calibration_params = {
                'gyro_scale_factor': np.ones(3) + np.random.normal(0, 0.001, 3),  # Scale factor error
                'gyro_cross_coupling': np.random.normal(0, 0.001, (3, 3)),       # Cross-axis sensitivity
                'accel_scale_factor': np.ones(3) + np.random.normal(0, 0.001, 3), # Scale factor error
                'accel_cross_coupling': np.random.normal(0, 0.001, (3, 3)),       # Cross-axis sensitivity
                'misalignment_matrix': np.eye(3) + np.random.normal(0, 1e-4, (3, 3)), # Misalignment
            }
        self.calibration_params = calibration_params

        # Internal state
        self.gyro_bias = self.bias_params['gyro_bias_init'].copy()
        self.accel_bias = self.bias_params['accel_bias_init'].copy()

        # Bias drift integrators
        self.gyro_bias_walk_integrator = np.zeros(3)
        self.accel_bias_walk_integrator = np.zeros(3)

        # Timing
        self.last_update_time = 0.0
        self.update_rate = 100  # Hz
        self.dt = 1.0 / self.update_rate

        # Temperature effects (simplified)
        self.temperature = 25.0  # degrees Celsius
        self.temperature_coefficients = {
            'gyro_temp_drift': 1e-6,  # rad/s/°C
            'accel_temp_drift': 1e-5, # m/s²/°C
        }

    def update(self, body_state, current_time):
        """
        Update IMU measurements based on body state
        """
        # Extract body state
        body_position = body_state['position']
        body_orientation = body_state['orientation']  # quaternion [w, x, y, z]
        body_angular_velocity = body_state['angular_velocity']  # in world frame
        body_linear_acceleration = body_state['linear_acceleration']  # in world frame

        # Calculate time since last update
        dt = current_time - self.last_update_time
        self.last_update_time = current_time

        # Transform body state to IMU frame
        imu_angular_velocity = self._transform_vector_to_imu_frame(
            body_angular_velocity, body_orientation
        )

        imu_linear_acceleration = self._transform_vector_to_imu_frame(
            body_linear_acceleration, body_orientation
        )

        # Add gravity to acceleration (IMU measures proper acceleration)
        gravity_world = np.array([0, 0, -9.81])
        gravity_imu = self._transform_vector_to_imu_frame(gravity_world, body_orientation)
        imu_linear_acceleration += gravity_imu

        # Apply systematic errors and biases
        gyro_measurement = self._apply_gyro_model(imu_angular_velocity, dt)
        accel_measurement = self._apply_accel_model(imu_linear_acceleration, dt)

        # Update bias random walks
        self._update_bias_walks(dt)

        return {
            'angular_velocity': gyro_measurement,
            'linear_acceleration': accel_measurement,
            'orientation': self.orientation,  # Could also integrate from gyro
            'temperature': self.temperature,
            'timestamp': current_time
        }

    def _transform_vector_to_imu_frame(self, vector, body_orientation):
        """
        Transform a vector from world frame to IMU frame
        """
        # Convert body orientation quaternion to rotation matrix
        # Note: scipy expects [x, y, z, w] format
        r_body = R.from_quat([body_orientation[1], body_orientation[2], body_orientation[3], body_orientation[0]])
        vector_body = r_body.inv().apply(vector)

        # Then transform to IMU frame (assuming IMU orientation relative to body)
        # For simplicity, assuming IMU orientation is the same as body orientation
        # In practice, this would use the IMU mounting orientation
        r_imu = R.from_quat([self.orientation[1], self.orientation[2], self.orientation[3], self.orientation[0]])
        vector_imu = r_imu.inv().apply(vector_body)

        return vector_imu

    def _apply_gyro_model(self, true_angular_velocity, dt):
        """
        Apply complete gyro measurement model
        """
        # Apply scale factor and cross-coupling calibration
        calibrated_input = (self.calibration_params['gyro_scale_factor'] * true_angular_velocity +
                           self.calibration_params['gyro_cross_coupling'] @ true_angular_velocity)

        # Add bias
        biased_measurement = calibrated_input + self.gyro_bias

        # Add temperature drift
        temp_drift = self.temperature_coefficients['gyro_temp_drift'] * (self.temperature - 25.0)
        biased_measurement += temp_drift

        # Add noise
        gyro_noise = self._generate_gyro_noise(dt)
        final_measurement = biased_measurement + gyro_noise

        return final_measurement

    def _apply_accel_model(self, true_linear_acceleration, dt):
        """
        Apply complete accelerometer measurement model
        """
        # Apply scale factor and cross-coupling calibration
        calibrated_input = (self.calibration_params['accel_scale_factor'] * true_linear_acceleration +
                           self.calibration_params['accel_cross_coupling'] @ true_linear_acceleration)

        # Add bias
        biased_measurement = calibrated_input + self.accel_bias

        # Add temperature drift
        temp_drift = self.temperature_coefficients['accel_temp_drift'] * (self.temperature - 25.0)
        biased_measurement += temp_drift

        # Add noise
        accel_noise = self._generate_accel_noise(dt)
        final_measurement = biased_measurement + accel_noise

        return final_measurement

    def _generate_gyro_noise(self, dt):
        """
        Generate gyro noise including white noise and random walk
        """
        # White noise component (ARW - Angle Random Walk)
        arw_std = self.noise_params['gyro_noise_density'] / np.sqrt(dt)
        white_noise = np.random.normal(0, arw_std, 3)

        # Random walk component (RRW - Rate Random Walk)
        # Integrate the random walk process
        rrw_std = self.noise_params['gyro_random_walk'] * np.sqrt(dt)
        random_walk_increment = np.random.normal(0, rrw_std, 3)
        self.gyro_bias_walk_integrator += random_walk_increment

        return white_noise + self.gyro_bias_walk_integrator

    def _generate_accel_noise(self, dt):
        """
        Generate accelerometer noise including white noise and random walk
        """
        # White noise component (VRW - Velocity Random Walk)
        vrw_std = self.noise_params['accel_noise_density'] / np.sqrt(dt)
        white_noise = np.random.normal(0, vrw_std, 3)

        # Random walk component
        random_walk_std = self.noise_params['accel_random_walk'] * np.sqrt(dt)
        random_walk_increment = np.random.normal(0, random_walk_std, 3)
        self.accel_bias_walk_integrator += random_walk_increment

        return white_noise + self.accel_bias_walk_integrator

    def _update_bias_walks(self, dt):
        """
        Update bias random walk processes
        """
        # Gyro bias random walk
        gyro_bias_drift = np.random.normal(
            0,
            self.bias_params['gyro_bias_walk'] * np.sqrt(dt),
            3
        )
        self.gyro_bias += gyro_bias_drift

        # Accel bias random walk
        accel_bias_drift = np.random.normal(
            0,
            self.bias_params['accel_bias_walk'] * np.sqrt(dt),
            3
        )
        self.accel_bias += accel_bias_drift

class MultiIMUSystem:
    def __init__(self, imu_configs):
        """
        Initialize multiple IMUs with different configurations
        """
        self.imus = []
        for config in imu_configs:
            imu = IMUSensor(**config)
            self.imus.append(imu)

    def update_all(self, body_states, current_time):
        """
        Update all IMUs with the same body state
        """
        measurements = []
        for imu in self.imus:
            measurement = imu.update(body_states, current_time)
            measurements.append(measurement)

        return measurements

    def get_sensor_fusion_input(self):
        """
        Format measurements for sensor fusion algorithms
        """
        gyro_measurements = np.zeros((len(self.imus), 3))
        accel_measurements = np.zeros((len(self.imus), 3))

        for i, measurement in enumerate(self.get_all_latest()):
            gyro_measurements[i] = measurement['angular_velocity']
            accel_measurements[i] = measurement['linear_acceleration']

        return {
            'gyro': gyro_measurements,
            'accel': accel_measurements
        }
```

## 4. Vision Sensor Simulation Implementation

### Camera Model and Image Formation

```python
import cv2
import numpy as np
from scipy.spatial.transform import Rotation as R

class CameraSensor:
    def __init__(self, width=640, height=480, fov=60,
                 noise_params=None, distortion_params=None):
        self.width = width
        self.height = height
        self.fov = fov  # Field of view in degrees

        # Calculate focal length from FOV
        self.focal_length = (self.width / 2) / np.tan(np.radians(fov / 2))
        self.cx = self.width / 2  # Principal point x
        self.cy = self.height / 2  # Principal point y

        # Intrinsic matrix
        self.intrinsic_matrix = np.array([
            [self.focal_length, 0, self.cx],
            [0, self.focal_length, self.cy],
            [0, 0, 1]
        ])

        # Noise parameters
        if noise_params is None:
            noise_params = {
                'photon_noise_factor': 0.01,    # Proportional to signal
                'dark_current_noise': 0.001,   # Constant noise
                'read_noise': 0.005,           # Electronics noise
                'quantization_noise': 0.001,   # ADC quantization
                'fixed_pattern_noise': 0.002,  # Pixel-to-pixel variations
            }
        self.noise_params = noise_params

        # Distortion parameters (Brown-Conrady model)
        if distortion_params is None:
            distortion_params = {
                'k1': 0.0,    # Radial distortion coefficient 1
                'k2': 0.0,    # Radial distortion coefficient 2
                'k3': 0.0,    # Radial distortion coefficient 3
                'p1': 0.0,    # Tangential distortion coefficient 1
                'p2': 0.0,    # Tangential distortion coefficient 2
            }
        self.distortion_params = distortion_params

        # Sensor physical parameters
        self.pixel_size = 5.5e-6  # 5.5 micrometers
        self.bit_depth = 12  # 12-bit ADC
        self.max_analog_value = 2**12 - 1

        # Exposure parameters
        self.exposure_time = 1.0/30  # 30 FPS default
        self.iso_sensitivity = 100

        # Motion blur parameters
        self.enable_motion_blur = True
        self.motion_blur_kernel_size = 3

        # Current image state
        self.current_image = None
        self.last_timestamp = 0.0

    def render_scene(self, scene_description, camera_pose):
        """
        Render a scene to generate synthetic image with realistic effects
        """
        # This would typically interface with a 3D renderer
        # For this implementation, we'll simulate the rendering process

        # Extract camera pose
        camera_position = camera_pose['position']
        camera_orientation = camera_pose['orientation']  # quaternion [w, x, y, z]

        # Create synthetic scene (in practice, this would come from a 3D renderer)
        synthetic_image = self._create_synthetic_scene(scene_description)

        # Apply camera effects
        distorted_image = self._apply_distortion(synthetic_image)
        noisy_image = self._add_noise(distorted_image)
        motion_blurred_image = self._apply_motion_blur(noisy_image, camera_pose)

        # Convert to appropriate format
        final_image = self._convert_to_sensor_format(motion_blurred_image)

        return final_image

    def _create_synthetic_scene(self, scene_description):
        """
        Create a synthetic image based on scene description
        """
        # Create blank image
        image = np.zeros((self.height, self.width, 3), dtype=np.float32)

        # For demonstration, create a simple test pattern
        # In practice, this would render a 3D scene
        for obj in scene_description.get('objects', []):
            if obj['type'] == 'sphere':
                center = self._world_to_pixel(obj['position'])
                radius = int(obj['radius'] * self.focal_length / obj['position'][2])  # Simplified
                color = obj['color']

                if 0 <= center[0] < self.width and 0 <= center[1] < self.height:
                    cv2.circle(image, center, radius, color, -1)

        return image

    def _world_to_pixel(self, world_point):
        """
        Convert 3D world point to 2D pixel coordinates
        """
        # Simple perspective projection
        if world_point[2] > 0:
            x = world_point[0] / world_point[2]
            y = world_point[1] / world_point[2]

            u = x * self.focal_length + self.cx
            v = y * self.focal_length + self.cy

            return int(u), int(v)
        else:
            return -1, -1  # Invalid projection

    def _apply_distortion(self, image):
        """
        Apply lens distortion to image
        """
        # Create coordinate grids
        y_coords, x_coords = np.mgrid[:self.height, :self.width]

        # Convert to normalized coordinates
        x_norm = (x_coords - self.cx) / self.focal_length
        y_norm = (y_coords - self.cy) / self.focal_length

        # Calculate distortion
        r_squared = x_norm**2 + y_norm**2
        radial_distortion = (1 +
                            self.distortion_params['k1'] * r_squared +
                            self.distortion_params['k2'] * r_squared**2 +
                            self.distortion_params['k3'] * r_squared**3)

        # Tangential distortion
        x_distorted = x_norm * radial_distortion + 2*self.distortion_params['p1']*x_norm*y_norm + \
                     self.distortion_params['p2']*(r_squared + 2*x_norm**2)
        y_distorted = y_norm * radial_distortion + 2*self.distortion_params['p2']*x_norm*y_norm + \
                     self.distortion_params['p1']*(r_squared + 2*y_norm**2)

        # Convert back to pixel coordinates
        x_distorted = x_distorted * self.focal_length + self.cx
        y_distorted = y_distorted * self.focal_length + self.cy

        # Remap image
        distorted_image = np.zeros_like(image)
        valid_mask = (x_distorted >= 0) & (x_distorted < self.width) & \
                     (y_distorted >= 0) & (y_distorted < self.height)

        # Use bilinear interpolation for remapping
        x_floor = np.floor(x_distorted).astype(int)
        y_floor = np.floor(y_distorted).astype(int)
        x_ceil = x_floor + 1
        y_ceil = y_floor + 1

        # Calculate interpolation weights
        wx = x_distorted - x_floor
        wy = y_distorted - y_floor

        # Apply bilinear interpolation
        for c in range(3):  # RGB channels
            for i in range(self.height):
                for j in range(self.width):
                    if valid_mask[i, j]:
                        xf, yf = x_floor[i, j], y_floor[i, j]
                        xc, yc = x_ceil[i, j], y_ceil[i, j]

                        # Bilinear interpolation
                        if xf >= 0 and xc < self.width and yf >= 0 and yc < self.height:
                            val = (image[yf, xf, c] * (1 - wx[i, j]) * (1 - wy[i, j]) +
                                   image[yf, xc, c] * wx[i, j] * (1 - wy[i, j]) +
                                   image[yc, xf, c] * (1 - wx[i, j]) * wy[i, j] +
                                   image[yc, xc, c] * wx[i, j] * wy[i, j])
                            distorted_image[i, j, c] = val

        return distorted_image

    def _add_noise(self, image):
        """
        Add realistic sensor noise to image
        """
        noisy_image = image.copy()

        # Photon noise (signal-dependent)
        photon_noise = np.random.normal(0, self.noise_params['photon_noise_factor'] * image)
        noisy_image += photon_noise

        # Dark current noise (constant)
        dark_noise = np.random.normal(0, self.noise_params['dark_current_noise'], image.shape)
        noisy_image += dark_noise

        # Read noise (electronics)
        read_noise = np.random.normal(0, self.noise_params['read_noise'], image.shape)
        noisy_image += read_noise

        # Fixed pattern noise (pixel-to-pixel variations)
        fpn = np.random.normal(0, self.noise_params['fixed_pattern_noise'], image.shape)
        noisy_image += fpn

        # Quantization noise
        # First normalize to [0, 1] then quantize
        normalized = np.clip(noisy_image, 0, 1)
        quantized = np.round(normalized * (2**self.bit_depth - 1)) / (2**self.bit_depth - 1)
        noisy_image = quantized

        return np.clip(noisy_image, 0, 1)

    def _apply_motion_blur(self, image, camera_pose):
        """
        Apply motion blur based on camera motion
        """
        if not self.enable_motion_blur:
            return image

        # Calculate motion blur kernel based on camera velocity
        # This is a simplified model
        if 'velocity' in camera_pose:
            velocity = camera_pose['velocity']
            motion_magnitude = np.linalg.norm(velocity)

            if motion_magnitude > 0.1:  # Only apply if significant motion
                # Create motion blur kernel
                kernel_size = max(1, int(motion_magnitude * self.exposure_time * 10))
                if kernel_size > 1:
                    kernel = np.zeros((kernel_size, kernel_size))
                    kernel[kernel_size//2, :] = 1.0 / kernel_size

                    # Apply motion blur to each channel
                    blurred = np.zeros_like(image)
                    for c in range(3):
                        blurred[:, :, c] = cv2.filter2D(image[:, :, c], -1, kernel)

                    return blurred

        return image

    def _convert_to_sensor_format(self, image):
        """
        Convert to final sensor output format
        """
        # Convert to uint8
        final_image = (image * 255).astype(np.uint8)
        return final_image

    def update(self, scene_description, camera_pose, current_time):
        """
        Update camera sensor with new scene
        """
        # Render the scene
        rendered_image = self.render_scene(scene_description, camera_pose)

        # Store current image and timestamp
        self.current_image = rendered_image
        self.last_timestamp = current_time

        return {
            'image': rendered_image,
            'timestamp': current_time,
            'camera_pose': camera_pose,
            'intrinsic_matrix': self.intrinsic_matrix,
            'distortion_coeffs': np.array([
                self.distortion_params['k1'], self.distortion_params['k2'],
                self.distortion_params['p1'], self.distortion_params['p2'],
                self.distortion_params['k3']
            ])
        }

class StereoCameraSystem:
    def __init__(self, baseline=0.2, camera_config=None):
        """
        Initialize stereo camera system
        """
        if camera_config is None:
            camera_config = {}

        # Left camera (reference)
        self.left_camera = CameraSensor(**camera_config)

        # Right camera (offset by baseline)
        right_config = camera_config.copy()
        right_config['position'] = np.array([baseline, 0, 0])  # Baseline offset
        self.right_camera = CameraSensor(**right_config)

        self.baseline = baseline

    def capture_stereo_pair(self, scene_description, camera_pose):
        """
        Capture synchronized stereo image pair
        """
        current_time = time.time()

        # Update both cameras with same scene and pose
        left_result = self.left_camera.update(scene_description, camera_pose, current_time)
        right_result = self.right_camera.update(scene_description, camera_pose, current_time)

        return {
            'left_image': left_result['image'],
            'right_image': right_result['image'],
            'timestamp': current_time,
            'baseline': self.baseline,
            'left_intrinsic': left_result['intrinsic_matrix'],
            'right_intrinsic': right_result['intrinsic_matrix'],
            'left_distortion': left_result['distortion_coeffs'],
            'right_distortion': right_result['distortion_coeffs']
        }

    def compute_disparity_map(self, stereo_pair):
        """
        Compute disparity map from stereo images
        """
        # Convert to grayscale
        left_gray = cv2.cvtColor(stereo_pair['left_image'], cv2.COLOR_RGB2GRAY)
        right_gray = cv2.cvtColor(stereo_pair['right_image'], cv2.COLOR_RGB2GRAY)

        # Create stereo matcher (simple block matching)
        stereo = cv2.StereoBM_create(numDisparities=64, blockSize=15)
        disparity = stereo.compute(left_gray, right_gray)

        # Convert to float and normalize
        disparity = disparity.astype(np.float32) / 16.0  # Divide by 16 to get true disparity

        return disparity
```

## 5. Range Sensor Simulation Implementation

### LIDAR Simulation

```python
class LIDARSensor:
    def __init__(self, fov_horizontal=360, fov_vertical=30,
                 resolution_horizontal=1080, resolution_vertical=64,
                 max_range=100.0, noise_params=None):
        # Configuration
        self.fov_horizontal = fov_horizontal  # degrees
        self.fov_vertical = fov_vertical      # degrees
        self.resolution_horizontal = resolution_horizontal
        self.resolution_vertical = resolution_vertical
        self.max_range = max_range

        # Angular resolution
        self.angle_step_h = fov_horizontal / resolution_horizontal
        self.angle_step_v = fov_vertical / resolution_vertical

        # Scan parameters
        self.horizontal_angles = np.linspace(
            -fov_horizontal/2, fov_horizontal/2, resolution_horizontal
        )
        self.vertical_angles = np.linspace(
            -fov_vertical/2, fov_vertical/2, resolution_vertical
        )

        # Noise parameters
        if noise_params is None:
            noise_params = {
                'distance_noise_std': 0.02,     # meters
                'angular_noise_std_h': 0.001,  # radians
                'angular_noise_std_v': 0.001,  # radians
                'intensity_noise_factor': 0.1, # proportional to return intensity
            }
        self.noise_params = noise_params

        # Physical parameters
        self.range_resolution = 0.01  # 1cm resolution
        self.scan_frequency = 10  # Hz
        self.return_intensity = True

        # Current scan state
        self.current_scan = None
        self.last_scan_time = 0.0

    def ray_trace(self, scene_description, lidar_pose):
        """
        Perform ray tracing to generate LIDAR scan
        """
        # Extract pose
        position = lidar_pose['position']
        orientation = lidar_pose['orientation']  # quaternion [w, x, y, z]

        # Convert quaternion to rotation matrix
        r = R.from_quat([orientation[1], orientation[2], orientation[3], orientation[0]])
        rotation_matrix = r.as_matrix()

        # Initialize scan arrays
        ranges = np.full((self.resolution_vertical, self.resolution_horizontal), self.max_range)
        intensities = np.zeros((self.resolution_vertical, self.resolution_horizontal))

        # For each beam
        for v_idx, v_angle in enumerate(self.vertical_angles):
            for h_idx, h_angle in enumerate(self.horizontal_angles):
                # Calculate beam direction in lidar frame
                theta = np.radians(h_angle)
                phi = np.radians(v_angle)

                # Direction vector in lidar frame
                beam_dir_local = np.array([
                    np.cos(phi) * np.cos(theta),
                    np.cos(phi) * np.sin(theta),
                    np.sin(phi)
                ])

                # Transform to world frame
                beam_dir_world = rotation_matrix @ beam_dir_local

                # Ray cast to find intersection
                hit_point, hit_normal, hit_intensity = self._ray_cast(
                    position, beam_dir_world, scene_description
                )

                if hit_point is not None:
                    # Calculate range
                    range_val = np.linalg.norm(hit_point - position)

                    # Apply noise
                    range_noise = np.random.normal(0, self.noise_params['distance_noise_std'])
                    ranges[v_idx, h_idx] = min(range_val + range_noise, self.max_range)

                    # Apply intensity noise
                    intensity_noise = np.random.normal(0, self.noise_params['intensity_noise_factor'] * hit_intensity)
                    intensities[v_idx, h_idx] = max(0, hit_intensity + intensity_noise)
                else:
                    ranges[v_idx, h_idx] = self.max_range
                    intensities[v_idx, h_idx] = 0

        return ranges, intensities

    def _ray_cast(self, origin, direction, scene_description):
        """
        Simple ray casting implementation
        """
        # For demonstration, implement a simple ray-object intersection
        # In practice, this would use an efficient ray tracing library

        min_distance = float('inf')
        hit_point = None
        hit_normal = None
        hit_intensity = 0

        for obj in scene_description.get('objects', []):
            if obj['type'] == 'plane':
                # Plane intersection: (p - p0) · n = 0
                p0 = obj['point']  # Point on plane
                n = obj['normal']  # Normal vector
                n = n / np.linalg.norm(n)  # Normalize

                # Ray: origin + t * direction
                # Plane: (point - p0) · n = 0
                denom = np.dot(n, direction)
                if abs(denom) > 1e-6:  # Not parallel
                    t = np.dot(p0 - origin, n) / denom
                    if t >= 0:  # Intersection in front
                        intersection = origin + t * direction
                        distance = np.linalg.norm(intersection - origin)

                        if distance < min_distance:
                            min_distance = distance
                            hit_point = intersection
                            hit_normal = n
                            hit_intensity = obj.get('reflectivity', 0.5)

            elif obj['type'] == 'sphere':
                # Sphere intersection: ||p - center||² = radius²
                center = obj['center']
                radius = obj['radius']

                # Ray equation: origin + t * direction
                # Substitute into sphere equation
                oc = origin - center
                a = np.dot(direction, direction)
                b = 2 * np.dot(oc, direction)
                c = np.dot(oc, oc) - radius**2

                discriminant = b**2 - 4*a*c
                if discriminant >= 0:
                    sqrt_disc = np.sqrt(discriminant)
                    t1 = (-b - sqrt_disc) / (2*a)
                    t2 = (-b + sqrt_disc) / (2*a)

                    # Take the closest positive intersection
                    t = min(t1, t2) if t1 >= 0 and t2 >= 0 else max(t1, t2) if t1 >= 0 or t2 >= 0 else -1

                    if t >= 0:
                        intersection = origin + t * direction
                        distance = np.linalg.norm(intersection - origin)

                        if distance < min_distance:
                            min_distance = distance
                            hit_point = intersection
                            # Normal at intersection point
                            hit_normal = (intersection - center) / radius
                            hit_intensity = obj.get('reflectivity', 0.5)

        return hit_point, hit_normal, hit_intensity

    def update(self, scene_description, lidar_pose, current_time):
        """
        Update LIDAR sensor with new scene
        """
        ranges, intensities = self.ray_trace(scene_description, lidar_pose)

        self.current_scan = {
            'ranges': ranges,
            'intensities': intensities,
            'horizontal_angles': self.horizontal_angles,
            'vertical_angles': self.vertical_angles,
            'timestamp': current_time
        }
        self.last_scan_time = current_time

        return self.current_scan

    def get_point_cloud(self, scan_data):
        """
        Convert range data to 3D point cloud
        """
        ranges = scan_data['ranges']
        angles_h = scan_data['horizontal_angles']
        angles_v = scan_data['vertical_angles']

        points = []
        for v_idx, v_angle in enumerate(angles_v):
            for h_idx, h_angle in enumerate(angles_h):
                range_val = ranges[v_idx, h_idx]

                if range_val < self.max_range * 0.9:  # Valid measurement
                    theta = np.radians(h_angle)
                    phi = np.radians(v_angle)

                    x = range_val * np.cos(phi) * np.cos(theta)
                    y = range_val * np.cos(phi) * np.sin(theta)
                    z = range_val * np.sin(phi)

                    points.append([x, y, z])

        return np.array(points)

class DepthCamera:
    def __init__(self, width=640, height=480, fov=60, max_depth=10.0):
        self.camera = CameraSensor(width=width, height=height, fov=fov)
        self.max_depth = max_depth
        self.depth_noise_std = 0.01  # meters

    def capture_depth_frame(self, scene_description, camera_pose):
        """
        Capture synchronized RGB and depth frames
        """
        current_time = time.time()

        # Get RGB image
        rgb_result = self.camera.update(scene_description, camera_pose, current_time)

        # Generate depth map (this would be done with ray tracing in practice)
        depth_map = self._generate_depth_map(scene_description, camera_pose)

        return {
            'rgb': rgb_result['image'],
            'depth': depth_map,
            'timestamp': current_time,
            'intrinsic_matrix': rgb_result['intrinsic_matrix']
        }

    def _generate_depth_map(self, scene_description, camera_pose):
        """
        Generate depth map from scene description
        """
        depth_map = np.full((self.camera.height, self.camera.width), self.max_depth, dtype=np.float32)

        # This is a simplified implementation
        # In practice, this would use ray tracing or stereo matching
        for obj in scene_description.get('objects', []):
            if obj['type'] == 'plane':
                # Calculate depth for each pixel by intersecting with plane
                for v in range(self.camera.height):
                    for u in range(self.camera.width):
                        # Convert pixel to ray
                        x = (u - self.camera.cx) / self.camera.focal_length
                        y = (v - self.camera.cy) / self.camera.focal_length

                        # Ray direction in camera frame
                        ray_dir_cam = np.array([x, y, 1.0])

                        # Transform to world frame
                        # This is simplified - would need proper transformation
                        # For demo purposes, assume camera is at origin looking along -Z
                        ray_dir_world = ray_dir_cam  # Simplified

                        # Plane intersection
                        p0 = obj['point']
                        n = obj['normal'] / np.linalg.norm(obj['normal'])

                        denom = np.dot(n, ray_dir_world)
                        if abs(denom) > 1e-6:
                            t = np.dot(p0, n) / denom
                            if t > 0:  # Intersection in front
                                depth_map[v, u] = min(t, self.max_depth)

        # Add noise
        noise = np.random.normal(0, self.depth_noise_std, depth_map.shape)
        depth_map += noise
        depth_map = np.clip(depth_map, 0, self.max_depth)

        return depth_map
```

## 6. Force/Torque Sensor Simulation

### Six-Axis Force/Torque Sensor

```python
class ForceTorqueSensor:
    def __init__(self, position=np.zeros(3), orientation=np.array([0, 0, 0, 1]),
                 noise_params=None, calibration_params=None, range_limits=None):
        self.position = position  # Position relative to parent link
        self.orientation = orientation  # Orientation relative to parent link

        # Noise parameters
        if noise_params is None:
            noise_params = {
                'force_noise_density': 0.1,      # N/sqrt(Hz)
                'torque_noise_density': 0.01,    # N*m/sqrt(Hz)
                'force_random_walk': 0.001,      # N/sqrt(s)
                'torque_random_walk': 0.0001,    # N*m/sqrt(s)
                'thermal_drift': 1e-5,           # N/°C
            }
        self.noise_params = noise_params

        # Calibration parameters
        if calibration_params is None:
            calibration_params = {
                'sensitivity_matrix': np.eye(6),  # 6x6 sensitivity matrix
                'cross_axis_coupling': np.zeros((6, 6)),  # Cross-axis effects
                'offset': np.zeros(6),  # Initial offset [Fx, Fy, Fz, Tx, Ty, Tz]
            }
        self.calibration_params = calibration_params

        # Range limits
        if range_limits is None:
            range_limits = {
                'max_force': 1000.0,    # N
                'max_torque': 100.0,    # N*m
            }
        self.range_limits = range_limits

        # Internal state
        self.bias = np.zeros(6)  # [Fx, Fy, Fz, Tx, Ty, Tz]
        self.bias_drift = np.zeros(6)
        self.temperature = 25.0  # °C

        # Integration for random walk
        self.force_rw_integrator = np.zeros(3)
        self.torque_rw_integrator = np.zeros(3)

        # Timing
        self.last_update_time = 0.0
        self.update_rate = 1000  # Hz
        self.dt = 1.0 / self.update_rate

    def update(self, applied_wrench, current_time):
        """
        Update force/torque sensor with applied wrench
        """
        dt = current_time - self.last_update_time
        self.last_update_time = current_time

        # Transform applied wrench to sensor frame if needed
        # For simplicity, assume wrench is already in sensor frame
        true_wrench = applied_wrench  # [Fx, Fy, Fz, Tx, Ty, Tz]

        # Apply calibration transformation
        calibrated_wrench = self._apply_calibration(true_wrench)

        # Add bias and drift
        biased_wrench = calibrated_wrench + self.bias + self.bias_drift

        # Add noise
        noisy_wrench = biased_wrench + self._generate_noise(dt)

        # Apply range limits
        measured_wrench = np.clip(
            noisy_wrench,
            [-self.range_limits['max_force']] * 3 + [-self.range_limits['max_torque']] * 3,
            [self.range_limits['max_force']] * 3 + [self.range_limits['max_torque']] * 3
        )

        # Update bias drift
        self._update_bias_drift(dt)

        return {
            'force': measured_wrench[:3],
            'torque': measured_wrench[3:],
            'raw_wrench': measured_wrench,
            'timestamp': current_time,
            'temperature': self.temperature
        }

    def _apply_calibration(self, true_wrench):
        """
        Apply sensor calibration
        """
        # Apply sensitivity matrix and cross-axis coupling
        calibrated = (self.calibration_params['sensitivity_matrix'] @ true_wrench +
                     self.calibration_params['cross_axis_coupling'] @ true_wrench)

        # Add offset
        calibrated += self.calibration_params['offset']

        return calibrated

    def _generate_noise(self, dt):
        """
        Generate sensor noise
        """
        noise = np.zeros(6)

        # Force noise
        force_white_std = self.noise_params['force_noise_density'] / np.sqrt(dt)
        force_white = np.random.normal(0, force_white_std, 3)

        force_rw_std = self.noise_params['force_random_walk'] * np.sqrt(dt)
        force_rw_increment = np.random.normal(0, force_rw_std, 3)
        self.force_rw_integrator += force_rw_increment

        noise[:3] = force_white + self.force_rw_integrator

        # Torque noise
        torque_white_std = self.noise_params['torque_noise_density'] / np.sqrt(dt)
        torque_white = np.random.normal(0, torque_white_std, 3)

        torque_rw_std = self.noise_params['torque_random_walk'] * np.sqrt(dt)
        torque_rw_increment = np.random.normal(0, torque_rw_std, 3)
        self.torque_rw_integrator += torque_rw_increment

        noise[3:] = torque_white + self.torque_rw_integrator

        return noise

    def _update_bias_drift(self, dt):
        """
        Update bias drift processes
        """
        # Simulate slow drift processes
        drift_rate = 1e-6  # Small drift per second
        drift_increment = np.random.normal(0, drift_rate * np.sqrt(dt), 6)
        self.bias_drift += drift_increment

    def calibrate(self, calibration_data):
        """
        Update calibration parameters using calibration data
        """
        # This would implement calibration algorithms
        # For now, just update the calibration parameters
        if 'sensitivity_matrix' in calibration_data:
            self.calibration_params['sensitivity_matrix'] = calibration_data['sensitivity_matrix']
        if 'offset' in calibration_data:
            self.calibration_params['offset'] = calibration_data['offset']

    def reset_calibration(self):
        """
        Reset to factory calibration
        """
        self.calibration_params = {
            'sensitivity_matrix': np.eye(6),
            'cross_axis_coupling': np.zeros((6, 6)),
            'offset': np.zeros(6),
        }

class MultiAxisForceSensorSystem:
    def __init__(self, sensor_configs):
        """
        Initialize multiple force/torque sensors
        """
        self.sensors = []
        for config in sensor_configs:
            sensor = ForceTorqueSensor(**config)
            self.sensors.append(sensor)

    def update_all(self, applied_wrenches, current_time):
        """
        Update all force/torque sensors
        """
        measurements = []
        for i, sensor in enumerate(self.sensors):
            measurement = sensor.update(applied_wrenches[i], current_time)
            measurements.append(measurement)

        return measurements

    def get_total_wrench(self, measurements):
        """
        Calculate total wrench from multiple sensors
        """
        total_force = np.zeros(3)
        total_torque = np.zeros(3)

        for measurement in measurements:
            total_force += measurement['force']
            total_torque += measurement['torque']

        return {
            'total_force': total_force,
            'total_torque': total_torque
        }
```

## 7. Sensor Fusion Implementation

### Kalman Filter for Sensor Fusion

```python
class ExtendedKalmanFilter:
    def __init__(self, n_states, n_observations):
        self.n_states = n_states
        self.n_observations = n_observations

        # State vector: [position, velocity, orientation, angular_velocity, biases, etc.]
        self.x = np.zeros(n_states)

        # State covariance matrix
        self.P = np.eye(n_states) * 1.0

        # Process noise covariance
        self.Q = np.eye(n_states) * 0.1

        # Measurement noise covariance (will be updated based on sensor models)
        self.R = np.eye(n_observations) * 1.0

        # Identity matrix
        self.I = np.eye(n_states)

    def predict(self, dt, process_model_func=None, control_input=None):
        """
        Prediction step of EKF
        """
        if process_model_func:
            # Use provided process model function
            self.x = process_model_func(self.x, dt, control_input)

            # Linearize process model to get Jacobian F
            F = self._linearize_process_model(process_model_func, self.x, dt, control_input)
        else:
            # Default: constant velocity model for position/velocity
            # This is a simplified example
            F = np.eye(self.n_states)

            # For a simple position-velocity model:
            if self.n_states >= 6:  # [px, py, pz, vx, vy, vz]
                for i in range(3):  # Update position based on velocity
                    F[i, i+3] = dt  # x = x + v*dt

        # Predict covariance
        self.P = F @ self.P @ F.T + self.Q

    def update(self, z, measurement_model_func, measurement_jacobian_func):
        """
        Update step of EKF
        """
        # Measurement function: h(x) gives expected measurement
        h_x = measurement_model_func(self.x)

        # Jacobian of measurement function: H = dh/dx
        H = measurement_jacobian_func(self.x)

        # Innovation
        y = z - h_x

        # Innovation covariance
        S = H @ self.P @ H.T + self.R

        # Kalman gain
        K = self.P @ H.T @ np.linalg.inv(S)

        # Update state
        self.x = self.x + K @ y

        # Update covariance (Joseph form for numerical stability)
        I_KH = self.I - K @ H
        self.P = I_KH @ self.P @ I_KH.T + K @ self.R @ K.T

        return self.x.copy()

    def _linearize_process_model(self, process_func, x, dt, u):
        """
        Linearize process model using finite differences
        """
        F = np.zeros((self.n_states, self.n_states))

        eps = 1e-8
        for i in range(self.n_states):
            x_plus = x.copy()
            x_minus = x.copy()
            x_plus[i] += eps
            x_minus[i] -= eps

            f_plus = process_func(x_plus, dt, u)
            f_minus = process_func(x_minus, dt, u)

            F[:, i] = (f_plus - f_minus) / (2 * eps)

        return F

class SensorFusionSystem:
    def __init__(self, robot_model):
        self.robot_model = robot_model

        # Initialize EKF for state estimation
        # State: [position, velocity, orientation, angular_velocity, IMU biases]
        self.ekf = ExtendedKalmanFilter(n_states=15, n_observations=12)  # Example: 6 IMU + 6 other

        # Sensor simulators
        self.imu = IMUSensor()
        self.encoders = MultiJointEncoderSystem(robot_model.n_joints)
        self.camera = CameraSensor()
        self.lidar = LIDARSensor()

        # Timing
        self.last_update_time = time.time()

    def predict_state(self, control_input, dt):
        """
        Predict state using robot dynamics model
        """
        def process_model(x, dt, u):
            # Simplified process model - in practice, use full robot dynamics
            new_x = x.copy()

            # Update positions based on velocities
            for i in range(3):  # Position components
                new_x[i] += x[i+3] * dt  # px += vx * dt

            # Update velocities based on accelerations (simplified)
            # This would use actual dynamics model in practice

            return new_x

        self.ekf.predict(dt, process_model, control_input)

    def update_with_measurements(self, sensor_measurements):
        """
        Update state estimate with sensor measurements
        """
        # Combine measurements from different sensors
        z_combined = self._combine_measurements(sensor_measurements)

        def measurement_model(x):
            # Map state to expected measurements
            # This would depend on sensor mounting positions and orientations
            return x[:self.ekf.n_observations]  # Simplified

        def measurement_jacobian(x):
            # Jacobian of measurement function
            H = np.zeros((self.ekf.n_observations, self.ekf.n_states))
            for i in range(self.ekf.n_observations):
                H[i, i] = 1.0  # Simplified: direct measurement of first n_observations states
            return H

        # Update EKF with combined measurements
        estimated_state = self.ekf.update(z_combined, measurement_model, measurement_jacobian)

        return estimated_state

    def _combine_measurements(self, sensor_measurements):
        """
        Combine measurements from different sensors
        """
        # This would properly align measurements from different sensors
        # taking into account their mounting positions and orientations
        combined = np.zeros(self.ekf.n_observations)

        # Example: combine IMU, encoder, etc. measurements
        # This is simplified - in practice, need proper coordinate transformations

        return combined

    def get_fused_state(self):
        """
        Get the current fused state estimate
        """
        return {
            'position': self.ekf.x[:3],
            'velocity': self.ekf.x[3:6],
            'orientation': self.ekf.x[6:10],  # quaternion
            'angular_velocity': self.ekf.x[10:13],
            'biases': self.ekf.x[13:15],
            'covariance': self.ekf.P
        }
```

## 8. Performance Optimization

### Efficient Sensor Simulation

```python
import numba
from numba import jit
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor

@jit(nopython=True)
def fast_encoder_simulation(true_position, resolution, noise_std, scale_error):
    """
    Fast encoder simulation using Numba JIT compilation
    """
    # Apply scale error
    measured_pos = (1 + scale_error) * true_position

    # Add noise
    noise = np.random.normal(0, noise_std)
    measured_pos += noise

    # Quantize
    counts = np.round(measured_pos * resolution / (2 * np.pi))
    quantized_pos = counts * (2 * np.pi) / resolution

    return quantized_pos

class OptimizedSensorSystem:
    def __init__(self, n_sensors=100):
        self.n_sensors = n_sensors

        # Pre-allocate arrays for efficiency
        self.positions = np.zeros(n_sensors)
        self.velocities = np.zeros(n_sensors)
        self.measurements = np.zeros(n_sensors)

        # Sensor parameters
        self.resolutions = np.full(n_sensors, 2**16)
        self.noise_stds = np.full(n_sensors, 1e-4)
        self.scale_errors = np.random.normal(0, 0.001, n_sensors)

    def update_batch(self, true_positions, true_velocities):
        """
        Update multiple sensors efficiently
        """
        for i in range(self.n_sensors):
            self.measurements[i] = fast_encoder_simulation(
                true_positions[i],
                self.resolutions[i],
                self.noise_stds[i],
                self.scale_errors[i]
            )

        return self.measurements.copy()

class ParallelSensorSimulator:
    def __init__(self, n_processes=None):
        self.n_processes = n_processes or mp.cpu_count()
        self.process_pool = None

    def __enter__(self):
        self.process_pool = mp.Pool(self.n_processes)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.process_pool:
            self.process_pool.close()
            self.process_pool.join()

    def simulate_sensors_parallel(self, sensor_configs, scene_descriptions, poses):
        """
        Simulate multiple sensors in parallel
        """
        if not self.process_pool:
            raise RuntimeError("Must use ParallelSensorSimulator as context manager")

        # Create sensor simulation tasks
        tasks = []
        for i, (config, scene_desc, pose) in enumerate(zip(sensor_configs, scene_descriptions, poses)):
            task = (i, config, scene_desc, pose)
            tasks.append(task)

        # Execute in parallel
        results = self.process_pool.map(self._simulate_single_sensor, tasks)

        # Sort results by sensor index
        sorted_results = [None] * len(results)
        for idx, result in results:
            sorted_results[idx] = result

        return sorted_results

    def _simulate_single_sensor(self, task):
        """
        Simulate a single sensor (used by worker processes)
        """
        idx, config, scene_desc, pose = task

        # Create appropriate sensor based on config
        if config.get('type') == 'camera':
            sensor = CameraSensor(**{k: v for k, v in config.items() if k != 'type'})
            result = sensor.update(scene_desc, pose, time.time())
        elif config.get('type') == 'lidar':
            sensor = LIDARSensor(**{k: v for k, v in config.items() if k != 'type'})
            result = sensor.update(scene_desc, pose, time.time())
        elif config.get('type') == 'imu':
            sensor = IMUSensor(**{k: v for k, v in config.items() if k != 'type'})
            result = sensor.update(pose, time.time())  # Simplified
        else:
            raise ValueError(f"Unknown sensor type: {config.get('type')}")

        return idx, result

class SensorDataBuffer:
    def __init__(self, buffer_size=1000):
        self.buffer_size = buffer_size
        self.buffers = {}
        self.timestamps = {}

    def add_data(self, sensor_id, data, timestamp):
        """
        Add sensor data to buffer
        """
        if sensor_id not in self.buffers:
            self.buffers[sensor_id] = []
            self.timestamps[sensor_id] = []

        self.buffers[sensor_id].append(data)
        self.timestamps[sensor_id].append(timestamp)

        # Trim buffer if too large
        if len(self.buffers[sensor_id]) > self.buffer_size:
            self.buffers[sensor_id] = self.buffers[sensor_id][-self.buffer_size:]
            self.timestamps[sensor_id] = self.timestamps[sensor_id][-self.buffer_size:]

    def get_recent_data(self, sensor_id, n_recent=10):
        """
        Get recent data from buffer
        """
        if sensor_id not in self.buffers:
            return [], []

        start_idx = max(0, len(self.buffers[sensor_id]) - n_recent)
        return (self.buffers[sensor_id][start_idx:],
                self.timestamps[sensor_id][start_idx:])

    def interpolate_data(self, sensor_id, target_time):
        """
        Interpolate sensor data to target time
        """
        if sensor_id not in self.buffers or len(self.buffers[sensor_id]) < 2:
            return None

        times = np.array(self.timestamps[sensor_id])
        idx_after = np.searchsorted(times, target_time)

        if idx_after == 0:
            return self.buffers[sensor_id][0]
        elif idx_after == len(times):
            return self.buffers[sensor_id][-1]
        else:
            # Linear interpolation
            t1, t2 = times[idx_after-1], times[idx_after]
            data1, data2 = self.buffers[sensor_id][idx_after-1], self.buffers[sensor_id][idx_after]

            alpha = (target_time - t1) / (t2 - t1)
            interpolated_data = data1 * (1 - alpha) + data2 * alpha

            return interpolated_data
```

## 9. Validation and Testing

### Sensor Model Validation

```python
class SensorValidator:
    def __init__(self, sensor_model):
        self.sensor_model = sensor_model
        self.validation_results = {}

    def validate_noise_characteristics(self, n_samples=10000):
        """
        Validate that sensor noise follows expected statistical properties
        """
        samples = []
        for _ in range(n_samples):
            # Generate sample with constant input
            measurement = self.sensor_model.update(
                true_position=0.0,
                true_velocity=0.0,
                current_time=time.time()
            )
            samples.append(measurement['position'])

        samples = np.array(samples)

        # Calculate statistics
        mean = np.mean(samples)
        std = np.std(samples)
        variance = np.var(samples)

        # Expected statistics (based on sensor model parameters)
        expected_mean = 0.0  # Should be zero for zero input
        expected_variance = self.sensor_model.noise_std**2

        # Statistical tests
        from scipy import stats

        # Kolmogorov-Smirnov test for normality
        ks_stat, ks_p_value = stats.kstest(samples, 'norm', args=(expected_mean, self.sensor_model.noise_std))

        # Shapiro-Wilk test for normality (better for smaller samples)
        sw_stat, sw_p_value = stats.shapiro(samples[:5000])  # Limited by Shapiro-Wilk

        self.validation_results['noise_validation'] = {
            'sample_mean': mean,
            'sample_std': std,
            'sample_variance': variance,
            'expected_mean': expected_mean,
            'expected_std': self.sensor_model.noise_std,
            'expected_variance': expected_variance,
            'ks_test_statistic': ks_stat,
            'ks_test_p_value': ks_p_value,
            'sw_test_statistic': sw_stat,
            'sw_test_p_value': sw_p_value,
            'noise_normality_pass': ks_p_value > 0.05,  # Pass if p > 0.05 (distribution not significantly different from normal)
            'mean_close_to_zero': abs(mean) < 3 * std / np.sqrt(n_samples)  # 99.7% confidence interval
        }

        return self.validation_results['noise_validation']

    def validate_bias_drift(self, test_duration=10.0, dt=0.001):
        """
        Validate bias drift behavior over time
        """
        start_time = time.time()
        current_time = start_time

        bias_values = []
        timestamps = []

        while current_time - start_time < test_duration:
            measurement = self.sensor_model.update(
                true_position=0.0,
                true_velocity=0.0,
                current_time=current_time
            )

            bias_values.append(measurement['bias'])
            timestamps.append(current_time)

            current_time += dt

        bias_values = np.array(bias_values)
        timestamps = np.array(timestamps)

        # Analyze bias drift characteristics
        time_intervals = np.diff(timestamps)
        bias_changes = np.diff(bias_values, axis=0)

        # Calculate drift rate statistics
        drift_rates = bias_changes / time_intervals[:, np.newaxis]
        mean_drift_rate = np.mean(drift_rates, axis=0)
        std_drift_rate = np.std(drift_rates, axis=0)

        self.validation_results['bias_drift_validation'] = {
            'duration': test_duration,
            'n_samples': len(bias_values),
            'mean_drift_rate': mean_drift_rate,
            'std_drift_rate': std_drift_rate,
            'expected_drift_rate': self.sensor_model.bias_drift_rate,
            'drift_within_expected_bounds': np.all(np.abs(mean_drift_rate) < 3 * std_drift_rate)
        }

        return self.validation_results['bias_drift_validation']

    def validate_dynamic_response(self, input_signal_func, test_duration=5.0, dt=0.001):
        """
        Validate sensor response to dynamic inputs
        """
        start_time = time.time()
        current_time = start_time

        input_values = []
        output_values = []
        timestamps = []

        while current_time - start_time < test_duration:
            # Generate input signal
            input_pos = input_signal_func(current_time)
            input_vel = (input_signal_func(current_time + 1e-6) - input_pos) / 1e-6  # Approximate derivative

            measurement = self.sensor_model.update(
                true_position=input_pos,
                true_velocity=input_vel,
                current_time=current_time
            )

            input_values.append(input_pos)
            output_values.append(measurement['position'])
            timestamps.append(current_time)

            current_time += dt

        input_values = np.array(input_values)
        output_values = np.array(output_values)
        timestamps = np.array(timestamps)

        # Calculate response characteristics
        from scipy import signal

        # Frequency response analysis
        freqs, response = signal.freqz(output_values - np.mean(output_values),
                                      input_values - np.mean(input_values),
                                      fs=1.0/dt)

        # Phase lag analysis
        phase_lag = np.angle(response)

        # Amplitude attenuation
        amplitude_ratio = np.abs(response)

        self.validation_results['dynamic_response'] = {
            'frequency_response': {
                'frequencies': freqs,
                'magnitude': amplitude_ratio,
                'phase': phase_lag
            },
            'bandwidth': self._calculate_bandwidth(freqs, amplitude_ratio),
            'phase_margin': self._calculate_phase_margin(phase_lag),
            'delay': self._calculate_delay(timestamps, input_values, output_values)
        }

        return self.validation_results['dynamic_response']

    def _calculate_bandwidth(self, freqs, magnitude_response, threshold_db=-3):
        """
        Calculate sensor bandwidth (frequency where response drops to threshold)
        """
        threshold_mag = 10**(threshold_db/20)  # Convert dB to magnitude
        cutoff_idx = np.where(magnitude_response < threshold_mag)[0]

        if len(cutoff_idx) > 0:
            return freqs[cutoff_idx[0]]
        else:
            return freqs[-1]  # Bandwidth extends beyond measured range

    def _calculate_phase_margin(self, phase_response):
        """
        Calculate phase margin (phase lag at 0dB crossing)
        """
        # Find frequency where magnitude crosses 0dB (magnitude = 1)
        unity_gain_idx = np.argmin(np.abs(np.abs(phase_response) - 1.0))
        return phase_response[unity_gain_idx]

    def _calculate_delay(self, timestamps, input_signal, output_signal):
        """
        Calculate time delay between input and output
        """
        # Cross-correlation to find delay
        correlation = signal.correlate(output_signal - np.mean(output_signal),
                                      input_signal - np.mean(input_signal))
        lags = signal.correlation_lags(len(output_signal), len(input_signal))

        delay_samples = lags[np.argmax(correlation)]
        delay_time = delay_samples * (timestamps[1] - timestamps[0])

        return delay_time

class MonteCarloSensorValidator:
    def __init__(self, sensor_model, n_trials=1000):
        self.sensor_model = sensor_model
        self.n_trials = n_trials

    def run_monte_carlo_validation(self, test_function, **kwargs):
        """
        Run Monte Carlo validation of sensor properties
        """
        results = []

        for trial in range(self.n_trials):
            result = test_function(**kwargs)
            results.append(result)

        # Calculate statistics over all trials
        if isinstance(results[0], dict):
            # Aggregate dictionary results
            aggregated = {}
            for key in results[0].keys():
                if isinstance(results[0][key], (int, float, np.number)):
                    values = [r[key] for r in results]
                    aggregated[key] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values),
                        'median': np.median(values)
                    }
                else:
                    aggregated[key] = results[0][key]  # Non-numeric, take first
        else:
            # Numeric results
            aggregated = {
                'mean': np.mean(results),
                'std': np.std(results),
                'min': np.min(results),
                'max': np.max(results),
                'median': np.median(results)
            }

        return aggregated
```

This implementation section provides practical examples of sensor simulation for humanoid robots, including joint encoders, IMUs, cameras, LIDAR, and force/torque sensors. The code demonstrates how theoretical sensor concepts are translated into executable implementations that can generate realistic sensor data for robot simulation and testing.